{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5026615a",
   "metadata": {},
   "source": [
    "# Computational Social Science Project #2 \n",
    "\n",
    "**Enter your Name:**Taylor Galdi\n",
    "\n",
    "*Semester:* Fall 2024\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fceabce6",
   "metadata": {},
   "source": [
    "Below we fill in some of the code you might use to answer some of the questions. Here are some additional resources for when you get stuck:\n",
    "* Code and documentation provided in the course notebooks  \n",
    "* [Markdown cheatsheet](https://github.com/adam-p/markdown-here/wiki/Markdown-Cheatsheet) to help with formatting the Jupyter notebook\n",
    "* Try Googling any errors you get and consult Stack Overflow, etc. Someone has probably had your question before!\n",
    "* Send me a pull request on GitHub flagging the syntax that's tripping you up "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4f76ede",
   "metadata": {},
   "source": [
    "**INSTRUCTIONS:** For this project, copy all of the files in the Project 2 folder in the course repo into a \"Project 2\" subfolder within the \"Computational Social Science Projects\" directory that you created for the first project. You will work on the project locally, push your project to GitHub, and submit a link to the GitHub repo on bCourses by the project deadline. Be sure the final submission is in the main branch, which is what I'll pull down and re-run to grade. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7920223d",
   "metadata": {},
   "source": [
    "## 1. Introduction/Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "013cd72b",
   "metadata": {},
   "source": [
    "#### a) Import relevant libraries\n",
    "Here are some libraries you will need to get started. Along the way you may need to add more. Best practice is to add them here at the top of the script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ca2b1ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# import libraries you might need here \n",
    "#-----------\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "# use random seed for consistent results \n",
    "np.random.seed(273)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c5b23b0",
   "metadata": {},
   "source": [
    "#### b) Read in and inspect data frame \n",
    "Read in the data frame and look at some of its attributes. Read in the data contained in the projoect folder: \"Diabetes with Population Info by County 2017.csv\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "171cd9e4",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Diabetes with Population Info by County 2017.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# read in and inspect data frame\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m#-----------\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Note that \"CountyFips\" needs to be a string so the leading 0 isn't dropped (only if you want to make choropleth map) \u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m diabetes \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDiabetes with Population Info by County 2017.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m, \n\u001b[1;32m      6\u001b[0m                        dtype\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCountyFIPS\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mstr\u001b[39m})\n",
      "File \u001b[0;32m/opt/anaconda3/envs/CSS/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/CSS/lib/python3.12/site-packages/pandas/io/parsers/readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m/opt/anaconda3/envs/CSS/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_engine(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/CSS/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m get_handle(\n\u001b[1;32m   1881\u001b[0m     f,\n\u001b[1;32m   1882\u001b[0m     mode,\n\u001b[1;32m   1883\u001b[0m     encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1884\u001b[0m     compression\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1885\u001b[0m     memory_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmemory_map\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[1;32m   1886\u001b[0m     is_text\u001b[38;5;241m=\u001b[39mis_text,\n\u001b[1;32m   1887\u001b[0m     errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding_errors\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1888\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage_options\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1889\u001b[0m )\n\u001b[1;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m/opt/anaconda3/envs/CSS/lib/python3.12/site-packages/pandas/io/common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[1;32m    874\u001b[0m             handle,\n\u001b[1;32m    875\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[1;32m    876\u001b[0m             encoding\u001b[38;5;241m=\u001b[39mioargs\u001b[38;5;241m.\u001b[39mencoding,\n\u001b[1;32m    877\u001b[0m             errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[1;32m    878\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    879\u001b[0m         )\n\u001b[1;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Diabetes with Population Info by County 2017.csv'"
     ]
    }
   ],
   "source": [
    "#\n",
    "# read in and inspect data frame\n",
    "#-----------\n",
    "# Note that \"CountyFips\" needs to be a string so the leading 0 isn't dropped (only if you want to make choropleth map) \n",
    "diabetes = pd.read_csv(\"Diabetes with Population Info by County 2017.csv\", \n",
    "                       dtype={\"CountyFIPS\": str}) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca77b1bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# look at shape\n",
    "#-----------\n",
    "# look at the dimensions of the diabetes data frame\n",
    "print('shape: ', diabetes.shape) \n",
    "#Wow, lots of features!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91328e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# set pandas parameters\n",
    "#-----------\n",
    "# tells pandas how many rows to display when printing so results don't get truncated\n",
    "pd.set_option('display.max_rows', 100)\n",
    "\n",
    "# look at the data types for each column in diabetes df (likely be located under each row bc column names are long)\n",
    "print('data types:', diabetes.dtypes)\n",
    "#Hmm okay so I see some floats and integers, along with objects. I'm assuming that some of these will need to be converted"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4e77f8b",
   "metadata": {},
   "source": [
    "Immediately, we see that some of the features that should be numeric (e.g., Diabetes_Number, Obesity_Number,  and Physical_Inactivity_Number) are not. We can check to see what the non-numeric values are in a column where we are expecting numeric information with a combination of `str.isnumeric()` and `unique()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72315139",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# identify non-numeric features\n",
    "#-----------\n",
    "# Return rows where the column \"Diabetes_Number\" is non-numeric and get the unique values of these rows\n",
    "# the \"~\" below in front of diabetes negates the str.isnumeric() so it only takes non-numeric values\n",
    "print(diabetes[~diabetes[\"Diabetes_Number\"].str.isnumeric()][\"Diabetes_Number\"].unique()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df1e47c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Now do the same as above, but for \"Obesity_Number\"\n",
    "#-----------\n",
    "print(diabetes[~diabetes[\"Obesity_Number\"].str.isnumeric()][\"Obesity_Number\"].unique()) \n",
    "#Why/how is 'Suppressed' different than 'No Data'?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d1e6e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Now do the same as above, but for \"Physical_Inactivity_Number\" \n",
    "#-----------\n",
    "\n",
    "print(diabetes[~diabetes[\"Physical_Inactivity_Number\"].str.isnumeric()][\"Physical_Inactivity_Number\"].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b705636f",
   "metadata": {},
   "source": [
    "These values (\"Suppresssed\" and \"No Data\") contained in the two respective columns are coercing these features to objects instead of them being  integers. Let's drop those rows in the next section, and also recode \"Physical_Inactivity_Number\" feature to be an integer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac2a442a",
   "metadata": {},
   "source": [
    "#### c. Recode variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c31fe962",
   "metadata": {},
   "source": [
    "Convert 'Diabetes_Number', 'Obesity_Number', and 'Physical_Inactivity_Number' to integers below so we can use them in our analysis. Also fill in the object type we want to recode 'sex and age_total population_65 years and over_sex ratio (males per 100 females)' too (you'll have to scroll all the way over to the right)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4acf34b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Recode variables\n",
    "#-----------\n",
    "\n",
    "# Diabetes\n",
    "# ----------\n",
    "# keep only useful info about our target feature, i.e., where diabetes_number not = 'Suppressed'\n",
    "# note that the inside reference to the diabetes df identifies the column, and the outer calls specific rows according to a condition \n",
    "diabetes = diabetes[diabetes['Diabetes_Number']!=\"Suppressed\"] \n",
    "\n",
    "# use the astype method on Diabetes_Number to convert it to an integer...if you are not sure, what does the astype() documentation tell you are possible arguments? \n",
    "diabetes[\"Diabetes_Number\"] = diabetes[\"Diabetes_Number\"].astype(int)\n",
    "\n",
    "# Obesity\n",
    "diabetes = diabetes[diabetes['Obesity_Number']!=\"No Data\"] \n",
    "diabetes[\"Obesity_Number\"] = diabetes[\"Obesity_Number\"].astype(int)\n",
    "\n",
    "\n",
    "# Physical Inactivity\n",
    "diabetes = diabetes[diabetes[\"Physical_Inactivity_Number\"]!=\"No Data\"] \n",
    "diabetes[\"Physical_Inactivity_Number\"] = diabetes[\"Physical_Inactivity_Number\"].astype(int)\n",
    "\n",
    "\n",
    "# Some final changes \n",
    "# ----------\n",
    "# 65+ sex ratio had one \"-\" in it so let's drop that row first\n",
    "diabetes = diabetes[diabetes['sex and age_total population_65 years and over_sex ratio (males per 100 females)']!= \"-\"]\n",
    "\n",
    "# change to numeric from string, since it originally included the \"-\", which made it a string\n",
    "# you'll have to decide whether to make it integer or float \n",
    "diabetes['sex and age_total population_65 years and over_sex ratio (males per 100 females)'] = diabetes['sex and age_total population_65 years and over_sex ratio (males per 100 females)'].astype(float\n",
    "                                                                                                                                                                                                )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8a6850c",
   "metadata": {},
   "source": [
    "We should probably scale our count variables to be proportional to county population. We create the list 'rc_cols' to select all the features we want to rescale, and then use the `.div()` method to avoid typing out every single column we want to recode. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ddb2299",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Scale to county populations\n",
    "#-----------\n",
    "\n",
    "# select count variables to recode to percentages; make sure we leave out ratios and our population variable \n",
    "# because these don't make sense to scale by population\n",
    "rc_cols = [col for col in diabetes.columns if col not in ['County', 'State', 'CountyFIPS', \n",
    "                                                        'sex and age_total population_65 years and over_sex ratio (males per 100 females)', \n",
    "                                                        'sex and age_total population_sex ratio (males per 100 females)', \n",
    "                                                        'sex and age_total population_18 years and over_sex ratio (males per 100 females)',  \n",
    "                                                        'race_total population']]\n",
    "           \n",
    "# recode all selected columns to numeric\n",
    "diabetes[rc_cols] = diabetes[rc_cols].apply(pd.to_numeric, errors='coerce') \n",
    "\n",
    "# divide all columns but those listed above by total population to calculate rates\n",
    "diabetes[rc_cols] = diabetes[rc_cols].div(diabetes['race_total population'], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d9e1fe2",
   "metadata": {},
   "source": [
    "Let's check our work. Are all rates bounded by 0 and 1 as expected? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f8f887",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# check\n",
    "#-----------\n",
    "# set pandas options\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "# inspect recoded values\n",
    "diabetes_summary = diabetes.describe().transpose() # note we use the transpose method rather than .T because this object is not a numpy array\n",
    "  \n",
    "# check recoding \n",
    "with pd.option_context('display.max_rows', 100, 'display.max_columns', None): \n",
    "    display(diabetes_summary.iloc[ : ,[0,1,3,7]]) # select which columns in the summary table we want to present"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2dcc070",
   "metadata": {},
   "source": [
    "#### d. Check for duplicate columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7acce0aa",
   "metadata": {},
   "source": [
    "There are a lot of columns in this data frame. Let's see if there are any are duplicates. Note that Pandas will not allow them to have the same exact column name, so they will likely be distinct on column name but will be copies otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "700163cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Check for duplicate columns\n",
    "#-----------\n",
    "# I used Google to figure this out, and adapted this example for our purposes:  \n",
    "# source: https://thispointer.com/how-to-find-drop-duplicate-columns-in-a-dataframe-python-pandas/ \n",
    "def getDuplicateColumns(df):\n",
    "    '''\n",
    "    Get a list of duplicate columns.\n",
    "    It will iterate over all the columns in dataframe and find the columns whose contents are duplicate.\n",
    "    :param df: Dataframe object\n",
    "    :return: List of columns whose contents are duplicates.\n",
    "    '''\n",
    "    duplicateColumnNames = set()\n",
    "    # Iterate over all the columns in dataframe\n",
    "    for x in range(df.shape[1]):\n",
    "        # Select column at xth index.\n",
    "        col = df.iloc[:, x]\n",
    "        # Iterate over all the columns in DataFrame from (x+1)th index till end\n",
    "        for y in range(x + 1, df.shape[1]):\n",
    "            # Select column at yth index.\n",
    "            otherCol = df.iloc[:, y]\n",
    "            # Check if two columns at x 7 y index are equal\n",
    "            if col.equals(otherCol):\n",
    "                duplicateColumnNames.add(df.columns.values[y])\n",
    "    return list(duplicateColumnNames)\n",
    "\n",
    "duplicateColumnNames = list(getDuplicateColumns(diabetes))\n",
    "print('Duplicate Columns are as follows: ')\n",
    "duplicateColumnNames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba3a52e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# drop columns from duplicates list\n",
    "#-----------\n",
    "# now drop list of duplicate features from our df using the .drop() method\n",
    "diabetes = diabetes.drop(columns=['sex and age_total population_65 years and over_1',\n",
    " 'sex and age_total population',\n",
    " 'sex and age_total population_18 years and over_1',\n",
    " 'race_total population_one race_1',\n",
    " 'race_total population_two or more races_1',\n",
    " 'hispanic or latino and race_total population'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a85ec3",
   "metadata": {},
   "source": [
    "Finally, there are many states accounted for the in dataset. If we convert this column to a categorical variable, and create dummies, it will create a rather sparse matrix (many 0s in our dataset) becuase there will be 49 dummy variables. One alternative is to classify each state to a larger US region and use that variable instead of state. The following code will do that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd769e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a dictionary mapping states to regions\n",
    "state_to_region = {\n",
    "    'Alabama': 'Southeast',\n",
    "    'Alaska': 'West',\n",
    "    'Arizona': 'West',\n",
    "    'Arkansas': 'South',\n",
    "    'California': 'West',\n",
    "    'Colorado': 'West',\n",
    "    'Connecticut': 'Northeast',\n",
    "    'Delaware': 'Northeast',\n",
    "    'District of Columbia': 'Southeast',\n",
    "    'Florida': 'Southeast',\n",
    "    'Georgia': 'Southeast',\n",
    "    'Hawaii': 'West',\n",
    "    'Idaho': 'West',\n",
    "    'Illinois': 'Midwest',\n",
    "    'Indiana': 'Midwest',\n",
    "    'Iowa': 'Midwest',\n",
    "    'Kansas': 'Midwest',\n",
    "    'Kentucky': 'South',\n",
    "    'Louisiana': 'South',\n",
    "    'Maine': 'Northeast',\n",
    "    'Maryland': 'Northeast',\n",
    "    'Massachusetts': 'Northeast',\n",
    "    'Michigan': 'Midwest',\n",
    "    'Minnesota': 'Midwest',\n",
    "    'Mississippi': 'South',\n",
    "    'Missouri': 'Midwest',\n",
    "    'Montana': 'West',\n",
    "    'Nebraska': 'Midwest',\n",
    "    'Nevada': 'West',\n",
    "    'New Hampshire': 'Northeast',\n",
    "    'New Jersey': 'Northeast',\n",
    "    'New Mexico': 'West',\n",
    "    'New York': 'Northeast',\n",
    "    'North Carolina': 'Southeast',\n",
    "    'North Dakota': 'Midwest',\n",
    "    'Ohio': 'Midwest',\n",
    "    'Oklahoma': 'South',\n",
    "    'Oregon': 'West',\n",
    "    'Pennsylvania': 'Northeast',\n",
    "    'Rhode Island': 'Northeast',\n",
    "    'South Carolina': 'Southeast',\n",
    "    'South Dakota': 'Midwest',\n",
    "    'Tennessee': 'South',\n",
    "    'Texas': 'South',\n",
    "    'Utah': 'West',\n",
    "    'Vermont': 'Northeast',\n",
    "    'Virginia': 'Southeast',\n",
    "    'Washington': 'West',\n",
    "    'West Virginia': 'South',\n",
    "    'Wisconsin': 'Midwest',\n",
    "    'Wyoming': 'West'\n",
    "}\n",
    "\n",
    "# Add a new 'Region' column based on the mapping\n",
    "diabetes['Region'] = diabetes['State'].map(state_to_region)\n",
    "\n",
    "# Print to verify'Region' column has been added\n",
    "diabetes\n",
    "\n",
    "#Save as its own dataframe for later\n",
    "diabetes_counties=diabetes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4007b8d1",
   "metadata": {},
   "source": [
    "## 2. Exploratory Data Analysis\n",
    "\n",
    "Make at least two figures (feel free to make more) and explain their relevance to the scientific problem. The goal here is to uncover interesting patterns in the data, learn more about the scope of the problem, and communicate these findings to your audience in clear ways."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dfb3ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# EDA #1 and interpretations in this section \n",
    "# This scatterplot looks at the relationship between Physical Inactivity and Diabetes Rates. \n",
    "sns.scatterplot(x = \"Physical_Inactivity_Number\", \n",
    "            y = \"Diabetes_Number\", \n",
    "            data = diabetes, \n",
    "            color = \"lightgreen\").set(title='Relationship between Physical Inactivity and Diabetes ')\n",
    "#Remember this is *inactivity* so, as we may expect, there seems to be a strong, positive association between our feature and outcome. I am assuming that 'Diabetes Number' is a reflection of the population with Diabetes. Moreover, it does not seem like we are differentiating between Type 1 and Type 2 diabetes, which feels important to note."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e31f770",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# EDA #2 and interpretations in this section \n",
    "# This barplot looks at the relationship between Regions of the United States and Diabetes Rates. \n",
    "sns.barplot(x = \"Region\",            \n",
    "            y = \"Diabetes_Number\", \n",
    "            data = diabetes, \n",
    "            hue=\"Region\", legend=False).set(title='Regional Variation in Diabetes Rates')\n",
    "#We see that the Southeast and South have higher proportions of people with diabetes. We might wonder what leads to this variation, especially because we see that 'Diabetes Number' is lower in the 'West.' \n",
    "#Could we imagine that this is linked to either different rates of physical activity, access to resources (i.e. public transportation, doctors, food deserts, etc.), or infrastructure?   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd57b38c-becf-4e7d-9b76-701d42a78702",
   "metadata": {},
   "outputs": [],
   "source": [
    "#EDA 3  and interpretations in this section\n",
    "#Since it seems like there's a relationship between geography and Diabetes_Number, let's try and unpack what's happening (to the extent that we can with this data. \n",
    "#This barplot looks at physical inactivity rates in different regions of the United States\n",
    "sns.barplot(x = \"Region\",            \n",
    "            y = \"Physical_Inactivity_Number\", \n",
    "            data = diabetes, \n",
    "            hue=\"Region\", legend=False).set(title='Regional Variation in Physical Inactivity')\n",
    "#Okay, it seems like there are regional differences in physical inactivity in ways that make sense given our regional differences in Diabetes_Number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae6a908d-a75f-4608-ac7b-a5a8f65d9cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EDA #5 and interpretations in this section\n",
    "# This scatterplot looks at the relationship between Obesity and Diabetes Rates\n",
    "sns.scatterplot(x = \"Obesity_Number\", \n",
    "            y = \"Diabetes_Number\", \n",
    "            data = diabetes, \n",
    "            color = \"purple\").set(title='Relationship between Obesity and Diabetes Rates')\n",
    "#Again, not super surprising that obesity would be related to diabetes, except once again, we don't seem to be distinguishing between Type 1 and Type 2, so there might not be as strong of a causal story here as we might first think..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdcc8a5d-50b2-4b59-8ae6-cb11e99f1327",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EDA #5 and interpretations in this section\n",
    "# This barplot looks at the relationship between Race and Diabetes. It does so in a crude way (Black vs. White people).\n",
    "import matplotlib.pyplot as plt\n",
    "fig = plt.figure()\n",
    "\n",
    "# set the figure parameters\n",
    "figure = plt.figure()                            # set the figure space\n",
    "figure.subplots_adjust(wspace = .8, hspace=.5)   # adjust the space in between figures \n",
    "\n",
    "# plot 1\n",
    "# ----------\n",
    "figure.add_subplot(1,   # sets the number of rows\n",
    "                   2,   # sets columns,\n",
    "                   1)   # specifies the following code is for the first plot  \n",
    "\n",
    "# specify barplot for White individuals\n",
    "sns.barplot(x=\"race_total population_one race_white\", \n",
    "            y=\"Diabetes_Number\", \n",
    "            data=diabetes,\n",
    "           hue=\"race_total population_one race_white\", legend=False).set_title(\"Rates of Diabetes White Individuals\")\n",
    "\n",
    "# ensure the x-axis is the same on both plots\n",
    "plt.xlim(-2000,6000)\n",
    "\n",
    "\n",
    "# plot 2\n",
    "# ----------\n",
    "figure.add_subplot(1,   # sets the number of rows\n",
    "                   2,   # sets columns,\n",
    "                   2)   # specifies the following code is for the second plot\n",
    "\n",
    "# specify barplot for Black individuals\n",
    "sns.barplot(x=\"race_total population_one race_black or african american\", \n",
    "            y=\"Diabetes_Number\", \n",
    "            data=diabetes,\n",
    "           hue=\"race_total population_one race_black or african american\", legend=False).set_title(\"Rates of Diabetes Black Individuals\")\n",
    "\n",
    "# ensure the x-axis is the same on both plots\n",
    "plt.xlim(-2000,6000)\n",
    "\n",
    "# show the plots\n",
    "plt.show()\n",
    "#This is both interesting and unhelpful at the same time. We can see variance both by race and by county, but there are too many counties to make super meaningful conections."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4df05b0",
   "metadata": {},
   "source": [
    "## 3. Prepare to Fit Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73492869",
   "metadata": {},
   "source": [
    "### 3.1 Finalize Data Set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67257021",
   "metadata": {},
   "source": [
    "We've already cleaned up the data, but we can make a few more adjustments before partitioning the data and training models. Let's recode 'Region' to be a categorical variable using `pd.get_dummies` and drop 'State'. Also, we'll drop 'County' because 'CountyFIPS' is already a unique identifier for the county. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74aa1676",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Drop and get dummies\n",
    "#-----------\n",
    "\n",
    "# create dummy features out of 'Region', which might be related to diabetes rates \n",
    "diabetes_clean = pd.get_dummies(diabetes, \n",
    "                               columns = [\"Region\"],  \n",
    "                               drop_first = True) # drop the first as a reference \n",
    "\n",
    "# drop 'County' and 'State' variables\n",
    "diabetes_clean = diabetes_clean.drop(labels = [\"County\", \"State\"],\n",
    "                               axis = 1) # which axis tells python we want to drop columns rather than index rows?\n",
    "                                        #Axis 1!\n",
    "\n",
    "# look at first 10 rows of new data frame \n",
    "diabetes_clean.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9494affc-348b-404a-81cd-86123e910a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#I see that the first 10 are from the Southeast, I want to check that our recoding worked for our whole sample \n",
    "# look at first 10 rows of new data frame \n",
    "diabetes_clean.tail(10)\n",
    "#Great, I see the last ten are from the West!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d9f02e8-55b1-4451-bac0-434647a20016",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('data types:', diabetes.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c768ecd6",
   "metadata": {},
   "source": [
    "### 3.2 Partition Data, Feature Selection, and Standardization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f305051f",
   "metadata": {},
   "source": [
    "Now, we will partition our data to prepare it for the training process. Ultimately we want to use a 60% train—20% validation—20% test in this case. More data in the training set lowers bias, but then increases variance in the validation/test sets. Balancing between bias and variance with choice of these set sizes is important as we want to ensure that there is enough data to train on to get good predictions, but also want to make sure our hold-out sets are representative enough.\n",
    "\n",
    "Work through partitioning the data into the test/train/validation sets in the chunks below. Be sure to that if you are using Ridge or LASSO, you Standardize the data. Where you do this in the workflow matters so be clear about where you are doing this and why. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b28c192",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Partition data\n",
    "#-----------\n",
    "\n",
    "# import library\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# create y dataframe \n",
    "y = diabetes_clean[\"Diabetes_Number\"]\n",
    "\n",
    "# create X dataframe (include everything except \"Diabetes_Number\", our target, \n",
    "# and \"race alone or in combination with one or more other races_total population\")\n",
    "X = diabetes_clean.drop([\"Diabetes_Number\",\"race alone or in combination with one or more other races_total population\" ], \n",
    "                axis = 1) # refers to columns\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56216906",
   "metadata": {},
   "source": [
    "Investigate whether there are any features that you should remove prior to spliting and model fitting. You may also consider using plots and relationships you found in the EDA stage for this question. Be sure to justify your logic. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c8015c9-5aa3-4ef6-94e6-cc005009c27e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Make a correlation matrix to see which features are correlated\n",
    "\n",
    "correlation_matrix=diabetes_clean.corr()\n",
    "\n",
    "plt.figure(figsize=(12,8))\n",
    "sns.heatmap(correlation_matrix, annot=True, fmt=\".2f\", cmap=\"coolwarm\")\n",
    "plt.title(\"Correlation Matrix\")\n",
    "plt.show()\n",
    "\n",
    "#This is pretty ugly, let's just look at the numbers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b323baee-bd73-4a8f-b56e-5ac6055c825f",
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetes_row = correlation_matrix.loc['Diabetes_Number']\n",
    "print(diabetes_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed567cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Feature selection\n",
    "# I am dropping these based on what I see in the correlation matrix (i.e. the association between Diabetes_Number and these features\n",
    "#Is there a cleaner way to do this?\n",
    "#-----------\n",
    "X = diabetes_clean.drop(columns=[\"race_total population_one race_american indian and alaska native_cherokee tribal grouping\",                                                                \n",
    "\"race_total population_one race_american indian and alaska native_chippewa tribal grouping\",                                                                \n",
    "\"race_total population_one race_american indian and alaska native_navajo tribal grouping\",                                                                  \n",
    "\"race_total population_one race_american indian and alaska native_sioux tribal grouping\",  \n",
    "\"race_total population_one race_asian_asian indian\",                                                                                                        \n",
    "\"race_total population_one race_asian_chinese\",                                                                                                             \n",
    "\"race_total population_one race_asian_filipino\",                                                                                                            \n",
    "\"race_total population_one race_asian_japanese\",                                                                                                            \n",
    "\"race_total population_one race_asian_korean\",                                                                                                              \n",
    "\"race_total population_one race_asian_vietnamese\",                                                                                                          \n",
    "\"race_total population_one race_asian_other asian\",                                                                                                                                                                                       \n",
    "\"race_total population_one race_native hawaiian and other pacific islander_native hawaiian\",                                                                \n",
    "\"race_total population_one race_native hawaiian and other pacific islander_guamanian or chamorro\",                                                         \n",
    "\"race_total population_one race_native hawaiian and other pacific islander_samoan\",                                                                         \n",
    "\"race_total population_one race_native hawaiian and other pacific islander_other pacific islander\",  \n",
    "\"race_total population_two or more races_white and black or african american\",                                                                              \n",
    "\"race_total population_two or more races_white and american indian and alaska native\",                                                                      \n",
    "\"race_total population_two or more races_white and asian\",                                                                                                  \n",
    "\"race_total population_two or more races_black or african american and american indian and alaska native\",               \n",
    "\"hispanic or latino and race_total population_hispanic or latino (of any race)_mexican\",                                                                    \n",
    "\"hispanic or latino and race_total population_hispanic or latino (of any race)_puerto rican\",                                                               \n",
    "\"hispanic or latino and race_total population_hispanic or latino (of any race)_cuban\",                                                                      \n",
    "\"hispanic or latino and race_total population_hispanic or latino (of any race)_other hispanic or latino\",                                                                             \n",
    "\"hispanic or latino and race_total population_not hispanic or latino_white alone\",                                                                          \n",
    "\"hispanic or latino and race_total population_not hispanic or latino_black or african american alone\",                                                      \n",
    "\"hispanic or latino and race_total population_not hispanic or latino_american indian and alaska native alone\",                                              \n",
    "\"hispanic or latino and race_total population_not hispanic or latino_asian alone\",                                                                          \n",
    "\"hispanic or latino and race_total population_not hispanic or latino_native hawaiian and other pacific islander alone\",                                     \n",
    "\"hispanic or latino and race_total population_not hispanic or latino_some other race alone\",                                                                \n",
    "\"hispanic or latino and race_total population_not hispanic or latino_two or more races\",                                                                    \n",
    "\"hispanic or latino and race_total population_not hispanic or latino_two or more races_two races including some other race\",                                \n",
    "\"hispanic or latino and race_total population_not hispanic or latino_two or more races_two races excluding some other race -  and three or more races\",   \n",
    "\"sex and age_total population_sex ratio (males per 100 females)\",                                                                                           \n",
    "\"sex and age_total population_under 5 years\",                                                                                                               \n",
    "\"sex and age_total population_5 to 9 years\",                                                                                                                \n",
    "\"sex and age_total population_10 to 14 years\",                                                                                                              \n",
    "\"sex and age_total population_15 to 19 years\",                                                                                                              \n",
    "\"sex and age_total population_20 to 24 years\",                                                                                                              \n",
    "\"sex and age_total population_25 to 34 years\",                                                                                                              \n",
    "\"sex and age_total population_35 to 44 years\",                                                                                                              \n",
    "\"sex and age_total population_45 to 54 years\",                                                                                                              \n",
    "\"sex and age_total population_55 to 59 years\",                                                                                                              \n",
    "\"sex and age_total population_60 to 64 years\",                                                                                                              \n",
    "\"sex and age_total population_65 to 74 years\",                                                                                                              \n",
    "\"sex and age_total population_75 to 84 years\",                                                                                                              \n",
    "\"sex and age_total population_85 years and over\",                                                                                                                                                                                                                                                                                                                     \n",
    "\"sex and age_total population_16 years and over\",                                                                                                           \n",
    "\"sex and age_total population_18 years and over\",                                                                                                           \n",
    "\"sex and age_total population_21 years and over\",                                                                                                           \n",
    "\"sex and age_total population_62 years and over\",                                                                                                           \n",
    "\"sex and age_total population_65 years and over\",                                                                                                                                                                                                            \n",
    "\"sex and age_total population_18 years and over_sex ratio (males per 100 females)\",\"Diabetes_Number\",\"race alone or in combination with one or more other races_total population\", \"citizen -  voting age population_citizen -  18 and over population_male\",\"citizen -  voting age population_citizen -  18 and over population_female\"], axis = 1)  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "398f0541",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Training/test split\n",
    "#-----------\n",
    "\n",
    "# set the random seed\n",
    "np.random.seed(10)\n",
    "\n",
    "\n",
    "# split the data so that it returns 4 values: X_train, X_test, y_train, y_test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,                 # specify training dataset\n",
    "                                                    y,                 # specify test dataset\n",
    "                                                    train_size=0.8,      # specify proportional split for training\n",
    "                                                    test_size=0.2)       # specify proportional split for test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "987b8243",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Validation split\n",
    "#-----------\n",
    "\n",
    "# train_test_split returns 4 values: X_train, X_test, y_train, y_test, so how do we create a 60-20-20 train-validate-test split? \n",
    "X_train, X_validate, y_train, y_validate =  train_test_split(X_train, \n",
    "                                                            y_train,\n",
    "                                                            train_size=0.75, \n",
    "                                                            test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6cc8a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Standardization\n",
    "#-----------\n",
    "# Given that we want to only standardize non-dichotomous variables, we need to find a \n",
    "# solution that will loop over only the columns we want to standardize. The code below\n",
    "# identifies all non-dichotomous variables in our dataset and only standardizes those.\n",
    "\n",
    "# load library and create instance of Standard Scaler \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "\n",
    "# identify non-dichotomous columns we want to transform\n",
    "columns = list(X_test.select_dtypes(include=['number']).loc[:, X_test.nunique() > 2])\n",
    "\n",
    "# use loop to transform training data for only columns we want to transform\n",
    "for column in columns:\n",
    "    X_train[column] = scaler.fit_transform(X_train[column].values.reshape(-1, 1)).flatten()\n",
    "\n",
    "# use loop to transform validation data for only columns we want to transform\n",
    "for column in columns:\n",
    "    X_validate[column] = scaler.fit_transform(X_validate[column].values.reshape(-1, 1)).flatten()\n",
    "\n",
    "# use loop to transform test data for only columns we want to transform\n",
    "for column in columns:\n",
    "    X_test[column] = scaler.fit_transform(X_test[column].values.reshape(-1, 1)).flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73e762cb",
   "metadata": {},
   "source": [
    "## 4. Train Models\n",
    "\n",
    "In this section, train your models. \n",
    "\n",
    "**Note that if you use Lasso, you will likely need to specify a very low penalty (e.g., an alpha of 0.001) because of convergence problems.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1836726",
   "metadata": {},
   "source": [
    "### 4.1 Describe models\n",
    "\n",
    "Detail the basic logic and assumptions underlying each model, its pros/cons, and why it is a plausible choice for this problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28b90463",
   "metadata": {},
   "source": [
    "### 4.2 Train models\n",
    "\n",
    "Train each model in the training set, and be sure to tune hyperparameters if appropriate. Report any relevant summary statistics from the training set, including how well each model fits the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d5fb8ca",
   "metadata": {},
   "source": [
    "#### Model 1:  (OLS Linear Regression) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f3818c2",
   "metadata": {},
   "source": [
    "**MODEL DESCRIPTION:**\n",
    "**OLS Linear Regression:**\n",
    "Linear regression is certainly not the most exciting method around, but it is still useful! If we are making policy recommendations about the implementation of a diabetes prevention plan, we need to understand the relationship between features in our dataset and Diabetes_Number. Linear regression assumes that there is an approximately linear relationship between  predictor variable and the outcome of interest. From our exploratory analyses, this may be reasonable to assume for several of our (potentially) key features. Our goal here is to estimate our coefficients to fit the data - we will do this using ordinary least squares (OLS) regression. Our hope is to minimize the residual sum of squares (RSS) by choosing our coefficients. As discussed here, OLS's (relative) simplicity has trade-offs. One of the most important is the assumption of linearity, but again, it seems reasonable to try OLS here at least initially. Another is a question of colinearity, which definitely could arise here. Variance is high with OLS but we get the added benefit of not worrying about bias in the same way that we do with Ridge, LASSO, etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9e89c70-a1c3-4927-b34c-efc5f19e8106",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load libraries \n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import Ridge, Lasso, LinearRegression\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# use magic function\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "# set style\n",
    "sns.set_style(\"darkgrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d3ddd2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Model 1 training\n",
    "# create a model\n",
    "lin_reg = LinearRegression()\n",
    "\n",
    "# fit the model\n",
    "lin_model = lin_reg.fit(X_train, y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c2667b-df2b-47cc-af4b-c6a75f4b1c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print model coefficients and intercept\n",
    "print(lin_model.coef_)\n",
    "print(lin_model.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4539bb4c-7813-48da-b3dc-871dfb7f1956",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dataframe with the coefficient and feature names for plotting\n",
    "lin_reg_data = pd.DataFrame([lin_model.coef_, X.columns]).T # make a dataframe from the arrays\n",
    "lin_reg_data.columns = ['Coefficient', 'Feature']           # add column names for clarity\n",
    "\n",
    "# plot\n",
    "ax = sns.barplot(x=\"Coefficient\",                           # add x \n",
    "                 y=\"Feature\",                               # add y\n",
    "                 data=lin_reg_data)                         # specify data\n",
    "\n",
    "ax.set_title(\"OLS Coefficients\")                            # set title\n",
    "plt.show()                                                  # show plot\n",
    "#I imagine that some of these smaller coefficients might be due to the overlapping categorization of these features \n",
    "#(i.e. 'race_total population_one race' is not as meaningful as 'race_total population_one race_white')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b2ba8be-f480-4627-a319-61d014a8628e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the OLS Coefficients to sort\n",
    "coef_df = pd.DataFrame({'Feature': X.columns, 'Coefficient':lin_model.coef_})\n",
    "\n",
    "# Sort the DataFrame by Coefficient from smallest to largest\n",
    "sorted_coef_df = coef_df.sort_values(by='Coefficient')\n",
    "\n",
    "# Display sorted coefficients\n",
    "print(sorted_coef_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d5ff61-efeb-4cce-be25-7fc267a062e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# see parameters for linear regression\n",
    "lin_reg.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5988697f",
   "metadata": {},
   "source": [
    "#### Model 2:  Ridge Regression "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "607a9ca0-aa72-41fa-8899-eea5caca5367",
   "metadata": {},
   "source": [
    "**MODEL DESCRIPTION:** \n",
    "**Ridge Regression:**\n",
    "Ridge regression is different from OLS because of its usage of lambda, a tuning parameter. However, it is similar because it uses coefficients to fit the data with the goal of reducing RSS. When we increase lambda, the ridge regression fit is less flexible which leads to less variance but more bias. We, of course, think about this tradeoff. Ridge could be advantageous in an instance like this where we think there might be colinearity between features - as our penalty helps to stabilize the estimate (again, an advantage over OLS). One disadvantage of Ridge is that it includes all features in its models, so if we decided to ultimately use Ridge, we would have to manually exclude features not of interest. We will have to standardize after splitting to avoid data leakage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcea9bce",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Model 2 training\n",
    "#-----------\n",
    "# make and fit a Ridge regression model\n",
    "ridge_reg = Ridge()                                              # create the model\n",
    "ridge_model = ridge_reg.fit(X_train, y_train)                    # fit the model\n",
    "\n",
    "# create a dataframe with the coefficient and feature names for plotting\n",
    "ridge_reg_data = pd.DataFrame([ridge_model.coef_, X.columns]).T  # create a dataframe from the estimates\n",
    "ridge_reg_data.columns = ['Coefficient', 'Feature']              # add column names for clarity\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d1d728b-fcc9-458e-bb52-c839f5e54a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the figure parameters \n",
    "figure = plt.figure()                            # set the figure space\n",
    "figure.subplots_adjust(wspace = .8, hspace=.5)   # adjust the space in between figures \n",
    "\n",
    "\n",
    "\n",
    "# specify barplot for Ridge\n",
    "sns.barplot(x=\"Coefficient\", \n",
    "            y=\"Feature\", \n",
    "            data=ridge_reg_data).set_title(\"Ridge Coefficients\")\n",
    "#Hmm okay as we expected, Ridge doesn't take out features even if their coefficients are close to zero. If we choose ridge, we'll have to manually take out those ourselves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd25ec7b-f32d-423f-82c1-abcf36575609",
   "metadata": {},
   "outputs": [],
   "source": [
    "coef_df = pd.DataFrame({'Feature': X.columns, 'Coefficient':ridge_model.coef_})\n",
    "\n",
    "# Sort the DataFrame by Coefficient from smallest to largest\n",
    "sorted_coef_df = coef_df.sort_values(by='Coefficient')\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "# Display sorted coefficients\n",
    "print(sorted_coef_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fb1f5e8",
   "metadata": {},
   "source": [
    "#### Model 3:  LASSO Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d5189e3-0fbb-4de8-a4e8-a6335984a37e",
   "metadata": {},
   "source": [
    "**MODEL DESCRIPTION:** \n",
    "**LASSO:**\n",
    "LASSO (Least Absolute Shrinkage and Selection Operator), like Ridge, also shrinks coefficient estimates, but the penalty is slightly different than Ridge. When we increase our lambda with LASSO, our penalty can shrink some of these coefficient estimates to be actually zero, which in effect helps us with feature selection. This of course introduces the bias-variance tradeoff, with increased bias and reduced variance. We will have to standardize after splitting to avoid data leakage. LASSO can be useful if we think there are different effects for different groups. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e375f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 3 training\n",
    "#-----------\n",
    "# create and fit the model\n",
    "lasso_reg = Lasso(alpha=0.001,max_iter=15000)  # note the hypterparameter tuning will not converge with max_iter < 15000\n",
    "lasso_model = lasso_reg.fit(X_train, \n",
    "                            y_train)\n",
    "\n",
    "# create a dataframe with the coefficient and feature names for plotting\n",
    "lasso_reg_data = pd.DataFrame([lasso_model.coef_, X.columns]).T  # create a dataframe from the estimates\n",
    "lasso_reg_data.columns = ['Coefficient', 'Feature']              # add column names for clarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84fb5327-d965-4482-aaad-e07ce61444ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify barplot for LASSO \n",
    "sns.barplot(x=\"Coefficient\", \n",
    "            y=\"Feature\", \n",
    "            data=lasso_reg_data).set_title(\"LASSO Coefficients\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b99fd2-23e3-409f-a9f1-a306d2be1feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "coef_df = pd.DataFrame({'Feature': X.columns, 'Coefficient':lasso_model.coef_})\n",
    "\n",
    "# Sort the DataFrame by Coefficient from smallest to largest\n",
    "sorted_coef_df = coef_df.sort_values(by='Coefficient')\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "# Display sorted coefficients\n",
    "print(sorted_coef_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1566e4d-6961-46c9-9122-c34a50827b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso_reg.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02711ee5",
   "metadata": {},
   "source": [
    "## 5. Validate and Refine Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a94df3f-44e6-4a6a-b2d1-edadbf8b2e29",
   "metadata": {},
   "source": [
    "### 5.1 Predict on the validation set\n",
    "Using each of the models you trained, predict outcomes in the validation set. Evaluate how well each model did."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9ff923f-ccda-49cc-aaf2-551e5a7fcfcf",
   "metadata": {},
   "source": [
    "### Linear Regression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c7738d9",
   "metadata": {},
   "source": [
    "### 5.1 Predict on the validation set\n",
    "Using each of the models you trained, predict outcomes in the validation set. Evaluate how well each model did."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e253a482",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Predict on validation data\n",
    "#-----------\n",
    "lin_pred = lin_model.predict(X_validate)\n",
    "\n",
    "# plot the residuals on a scatter plot\n",
    "plt.scatter(y_validate, lin_pred)                    # specify x and y of the scatter plot\n",
    "plt.title('Linear Model (OLS) Predicted v. Actual')  # specify plot title\n",
    "plt.xlabel('actual value')                           # specify x-axis label\n",
    "plt.ylabel('predicted value')                        # specify y-axis label\n",
    "\n",
    "#Make a line of fit\n",
    "min_val = min(y_validate.min(), lin_pred.min())\n",
    "max_val = max(y_validate.max(), lin_pred.max())\n",
    "plt.plot([min_val, max_val], [min_val, max_val], color='red', linestyle='--', label='Perfect Prediction')\n",
    "\n",
    "# Optionally add a legend\n",
    "plt.legend()\n",
    "\n",
    "# Add a grid\n",
    "plt.grid()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb48071-8aad-4e35-83cf-db8f8ed023ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create function to calculate the root mean squared errror\n",
    "def rmse(pred, actual):\n",
    "    return np.sqrt(np.mean((pred - actual) ** 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff6681b-7c37-48de-8e3c-2b8073924d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate root mean squared error for OLS\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "rmse = np.sqrt(mean_squared_error(y_validate, lin_pred))\n",
    "print(\"RMSE is:\", rmse)\n",
    "#calculate r2 value\n",
    "r_squared = r2_score(y_validate, lin_pred)\n",
    "print(f'R-squared: {r_squared:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f4f276d-6b9a-4e1f-ba5e-616276d13e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "#While our RMSE doesn't seem terribly high, meaning our predicted values didn't deviate from our actual values an egregious amount\n",
    "#our R-squared value is pretty low. As we might've expected, OLS might not be our best-performing model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12efdc25",
   "metadata": {},
   "source": [
    "### RIDGE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3445088-4257-409c-8d17-2bf33af69e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the model to make predictions\n",
    "ridge_pred = ridge_model.predict(X_validate)\n",
    "\n",
    "# plot the predictions\n",
    "plt.scatter(y_validate, ridge_pred)\n",
    "plt.title('Ridge Model')\n",
    "plt.xlabel('actual values')\n",
    "plt.ylabel('predicted values')\n",
    "\n",
    "#Make a line of fit\n",
    "min_val = min(y_validate.min(), ridge_pred.min())\n",
    "max_val = max(y_validate.max(), ridge_pred.max())\n",
    "plt.plot([min_val, max_val], [min_val, max_val], color='red', linestyle='--', label='Perfect Prediction')\n",
    "\n",
    "# Optionally add a legend\n",
    "plt.legend()\n",
    "\n",
    "# Add a grid\n",
    "plt.grid()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a4f0ab-d6a7-41fb-82ae-a4fd4c9696b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the rmse for the Ridge model\n",
    "rmse = np.sqrt(mean_squared_error(y_validate, ridge_pred))\n",
    "print(\"RMSE is:\", rmse)\n",
    "#calculate r2 value\n",
    "r_squared = r2_score(y_validate, ridge_pred)\n",
    "print(f'R-squared: {r_squared:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "482bf1d6-53e5-423f-ade6-69f15b284860",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Okay our RMSE is a bit lower than OLS, which we can see visually, but our R-Squared is still pretty low. Let's hope that the LASSO can overcome this."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "539ae851-64b8-4925-9986-b1ff496a84b1",
   "metadata": {},
   "source": [
    "### LASSO \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9af21b21-de4f-4971-add8-5576967ffff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the model to make predictions\n",
    "lasso_pred = lasso_model.predict(X_validate)\n",
    "\n",
    "# plot the predictions\n",
    "plt.scatter(y_validate, lasso_pred)\n",
    "\n",
    "# add title and labels\n",
    "plt.title('LASSO Model')\n",
    "plt.xlabel('actual values')\n",
    "plt.ylabel('predicted values')\n",
    "#Make a line of fit\n",
    "min_val = min(y_validate.min(), lasso_pred.min())\n",
    "max_val = max(y_validate.max(), lasso_pred.max())\n",
    "plt.plot([min_val, max_val], [min_val, max_val], color='red', linestyle='--', label='Perfect Prediction')\n",
    "\n",
    "# Optionally add a legend\n",
    "plt.legend()\n",
    "\n",
    "# Add a grid\n",
    "plt.grid()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "715b7f91-8e00-4cb2-a2a9-d58789eabfd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the rmse for the LASSO model\n",
    "rmse = np.sqrt(mean_squared_error(y_validate, lasso_pred))\n",
    "print(\"RMSE is:\", rmse)\n",
    "#calculate r2 value\n",
    "r_squared = r2_score(y_validate, lasso_pred)\n",
    "print(f'R-squared: {r_squared:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba980e6c-92ca-42e0-8454-3fa6865f58f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Okay our RMSE is a bit lower than ridge and our R-squared is slightly higher, so let's go with LASSO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4320653-4de6-45b8-a340-9ab09d145235",
   "metadata": {},
   "source": [
    "### 5.2 Predict on the test set\n",
    "\n",
    "Now, choose your best performing model of the three, select out unimportant feature(s), retrain the model, and then predict on the test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0089bfa8-b72b-4dee-8fbf-22a02f2e5e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Feature selection\n",
    "#After loooking at LASSO coefficients above, it seems like a good idea to select out some more features \n",
    "\n",
    "#-----------\n",
    "X = diabetes_clean.drop(columns=[\"race_total population_one race_american indian and alaska native_cherokee tribal grouping\",                                                                \n",
    "\"race_total population_one race_american indian and alaska native_chippewa tribal grouping\",                                                                \n",
    "\"race_total population_one race_american indian and alaska native_navajo tribal grouping\",                                                                  \n",
    "\"race_total population_one race_american indian and alaska native_sioux tribal grouping\",  \n",
    "\"race_total population_one race_asian_asian indian\",                                                                                                        \n",
    "\"race_total population_one race_asian_chinese\",                                                                                                             \n",
    "\"race_total population_one race_asian_filipino\",                                                                                                            \n",
    "\"race_total population_one race_asian_japanese\",                                                                                                            \n",
    "\"race_total population_one race_asian_korean\",                                                                                                              \n",
    "\"race_total population_one race_asian_vietnamese\",                                                                                                          \n",
    "\"race_total population_one race_asian_other asian\",                                                                                                         \n",
    "\"race_total population_one race_native hawaiian and other pacific islander\",                                                                                \n",
    "\"race_total population_one race_native hawaiian and other pacific islander_native hawaiian\",                                                                \n",
    "\"race_total population_one race_native hawaiian and other pacific islander_guamanian or chamorro\",                                                         \n",
    "\"race_total population_one race_native hawaiian and other pacific islander_samoan\",                                                                         \n",
    "\"race_total population_one race_native hawaiian and other pacific islander_other pacific islander\",  \n",
    "\"race_total population_two or more races_white and black or african american\",                                                                              \n",
    "\"race_total population_two or more races_white and american indian and alaska native\",                                                                      \n",
    "\"race_total population_two or more races_white and asian\",                                                                                                  \n",
    "\"race_total population_two or more races_black or african american and american indian and alaska native\",               \n",
    "\"hispanic or latino and race_total population_hispanic or latino (of any race)_mexican\",                                                                    \n",
    "\"hispanic or latino and race_total population_hispanic or latino (of any race)_puerto rican\",                                                               \n",
    "\"hispanic or latino and race_total population_hispanic or latino (of any race)_cuban\",                                                                      \n",
    "\"hispanic or latino and race_total population_hispanic or latino (of any race)_other hispanic or latino\",                                                                             \n",
    "\"hispanic or latino and race_total population_not hispanic or latino_white alone\",                                                                          \n",
    "\"hispanic or latino and race_total population_not hispanic or latino_black or african american alone\",                                                      \n",
    "\"hispanic or latino and race_total population_not hispanic or latino_american indian and alaska native alone\",                                              \n",
    "\"hispanic or latino and race_total population_not hispanic or latino_asian alone\",                                                                          \n",
    "\"hispanic or latino and race_total population_not hispanic or latino_native hawaiian and other pacific islander alone\",                                     \n",
    "\"hispanic or latino and race_total population_not hispanic or latino_some other race alone\",                                                                \n",
    "\"hispanic or latino and race_total population_not hispanic or latino_two or more races\",                                                                    \n",
    "\"hispanic or latino and race_total population_not hispanic or latino_two or more races_two races including some other race\",                                \n",
    "\"hispanic or latino and race_total population_not hispanic or latino_two or more races_two races excluding some other race -  and three or more races\",   \n",
    "\"sex and age_total population_sex ratio (males per 100 females)\",                                                                                           \n",
    "\"sex and age_total population_under 5 years\",                                                                                                               \n",
    "\"sex and age_total population_5 to 9 years\",                                                                                                                \n",
    "\"sex and age_total population_10 to 14 years\",                                                                                                              \n",
    "\"sex and age_total population_15 to 19 years\",                                                                                                              \n",
    "\"sex and age_total population_20 to 24 years\",                                                                                                              \n",
    "\"sex and age_total population_25 to 34 years\",                                                                                                              \n",
    "\"sex and age_total population_35 to 44 years\",                                                                                                              \n",
    "\"sex and age_total population_45 to 54 years\",                                                                                                              \n",
    "\"sex and age_total population_55 to 59 years\",                                                                                                              \n",
    "\"sex and age_total population_60 to 64 years\",                                                                                                              \n",
    "\"sex and age_total population_65 to 74 years\",                                                                                                              \n",
    "\"sex and age_total population_75 to 84 years\",                                                                                                              \n",
    "\"sex and age_total population_85 years and over\",                                                                                                                                                                                                                                                                                                                     \n",
    "\"sex and age_total population_16 years and over\",                                                                                                           \n",
    "\"sex and age_total population_18 years and over\",                                                                                                           \n",
    "\"sex and age_total population_21 years and over\",                                                                                                           \n",
    "\"sex and age_total population_62 years and over\",                                                                                                           \n",
    "\"sex and age_total population_65 years and over\",                                                                                                                                                                                                            \n",
    "\"sex and age_total population_18 years and over_sex ratio (males per 100 females)\",\"Diabetes_Number\",\"race alone or in combination with one or more other races_total population\", \"citizen -  voting age population_citizen -  18 and over population_male\",\"citizen -  voting age population_citizen -  18 and over population_female\", \"race_total population\",                                                                                                                                      \n",
    "\"race_total population_one race\",                                                                                                                           \n",
    "\"race_total population_two or more races\",                                                                                                      \n",
    " \"CountyFIPS\", \n",
    "\"total housing units\",\"race_total population_one race_black or african american\",\n",
    "                                 \"sex and age_total population_65 years and over_sex ratio (males per 100 females)\",\n",
    "                                \"sex and age_total population_18 years and over_male\",\n",
    "                                \"sex and age_total population_18 years and over_female\",], axis = 1)  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6002238b-bbfe-4d8e-b777-e3d661346406",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 3 retraining\n",
    "#-----------\n",
    "# create and fit the model\n",
    "lasso_reg = Lasso(alpha=0.001,max_iter=15000)  # note the hypterparameter tuning will not converge with max_iter < 15000\n",
    "lasso_model = lasso_reg.fit(X_test, \n",
    "                            y_test)\n",
    "\n",
    "# create a dataframe with the coefficient and feature names for plotting\n",
    "lasso_reg_data = pd.DataFrame([lasso_model.coef_, X.columns]).T  # create a dataframe from the estimates\n",
    "lasso_reg_data.columns = ['Coefficient', 'Feature']              # add column names for clarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f8ef69-3d59-4345-9582-6109813df79c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify barplot for LASSO \n",
    "sns.barplot(x=\"Coefficient\", \n",
    "            y=\"Feature\", \n",
    "            data=lasso_reg_data).set_title(\"LASSO Coefficients\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d519b80-2814-49d5-824a-b35a0d430f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_validate_scaled = scaler.transform(X_validate)\n",
    "\n",
    "# Initialize Lasso model\n",
    "lasso_reg = Lasso(max_iter=15000)\n",
    "\n",
    "# Specify the hyperparameters for grid search\n",
    "param_grid = {\n",
    "    'alpha': np.logspace(-4, 0, 10),  # Wide range of alpha values\n",
    "    'fit_intercept': [True, False],\n",
    "    'selection': ['cyclic', 'random']\n",
    "}\n",
    "\n",
    "# Execute the grid search\n",
    "lasso_grid_reg = GridSearchCV(lasso_reg, param_grid, cv=5)  # 5-fold cross-validation\n",
    "\n",
    "# Fit the grid search on the scaled training data\n",
    "lasso_grid_reg.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Get the best estimator\n",
    "lasso_model = lasso_grid_reg.best_estimator_\n",
    "\n",
    "# Predictions\n",
    "best_lasso_pred = lasso_model.predict(X_validate_scaled)\n",
    "\n",
    "# Calculate metrics\n",
    "r_squared = r2_score(y_validate, best_lasso_pred)\n",
    "rmse_value = np.sqrt(mean_squared_error(y_validate, best_lasso_pred))\n",
    "\n",
    "print('Best CV R^2:', max(lasso_grid_reg.cv_results_[\"mean_test_score\"]))\n",
    "print('Validation R^2:', r_squared)\n",
    "print('Validation RMSE:', rmse_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79689b2a",
   "metadata": {},
   "source": [
    "### 5.3 Impement a cross-validation approach\n",
    "\n",
    "Finally, implement a cross-validation approach for your best model and evaluate the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "fcc70c94-26c2-4288-867c-f649bf0f4428",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best CV R^2: 0.49576481566608666\n",
      "Validation R^2: 0.4520486138844021\n",
      "Validation RMSE: 0.019265235472356777\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_validate_scaled = scaler.transform(X_validate)\n",
    "\n",
    "# Initialize Lasso model\n",
    "lasso_reg = Lasso(max_iter=15000)\n",
    "\n",
    "# Specify the hyperparameters for grid search\n",
    "param_grid = {\n",
    "    'alpha': np.logspace(-4, 0, 10),  # Wide range of alpha values\n",
    "    'fit_intercept': [True, False],\n",
    "    'selection': ['cyclic', 'random']\n",
    "}\n",
    "\n",
    "# Execute the grid search\n",
    "lasso_grid_reg = GridSearchCV(lasso_reg, param_grid, cv=5)  # 5-fold cross-validation\n",
    "\n",
    "# Fit the grid search on the scaled training data\n",
    "lasso_grid_reg.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Get the best estimator\n",
    "lasso_model = lasso_grid_reg.best_estimator_\n",
    "\n",
    "# Predictions\n",
    "best_lasso_pred = lasso_model.predict(X_validate_scaled)\n",
    "\n",
    "# Calculate metrics\n",
    "r_squared = r2_score(y_validate, best_lasso_pred)\n",
    "rmse_value = np.sqrt(mean_squared_error(y_validate, best_lasso_pred))\n",
    "\n",
    "print('Best CV R^2:', max(lasso_grid_reg.cv_results_[\"mean_test_score\"]))\n",
    "print('Validation R^2:', r_squared)\n",
    "print('Validation RMSE:', rmse_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "429d2f79-1baf-4a66-8de6-bd54be33f996",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Well, LASSO performed the best and it still wasn't great. Our Best Cross-Validation R-Squared value is still pretty low, as is our Validation R-Squared\n",
    "#So a lot of our variation is not explained by the model\n",
    "#However, our RMSE is lower!\n",
    "#We improved our model, but not by much."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "250994bb",
   "metadata": {},
   "source": [
    "## 6. Discussion Questions\n",
    "\n",
    "In this section, insert responses for discussion questions here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de2fb212",
   "metadata": {},
   "source": [
    "1. What is bias-variance tradeoff? Why is it relevant to machine learning problems like this one?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b143ca1a",
   "metadata": {},
   "source": [
    "**Question 1**\n",
    "The bias-variance tradeoff refers to the issue (errors) we encounter in machine learning. When we are fitting our models, we must consider how our models both introduce and combat these two errors and how this relates to our ability to make accurate predictions outside of our dataset. Bias relates to the relationship between our predicted and actual outputs; with more bias, we risk underfitting our model, meaning our algorithm can miss important relationships between variables. On the other hand, when we overfit the model, or, when our model becomes too sensitive to the noise in our training data. Both underfitting and overfitting can lead to issues when we try and predict out-of-sample. This tension between bias-variance is fundamental in machine learning and is a major factor that we take into consideration when choosing and evaluating our models. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b6a596d",
   "metadata": {},
   "source": [
    "2. Define overfitting, and why it matters for machine learning. How can we address it?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26bfab1f",
   "metadata": {},
   "source": [
    "**YOUR ANSWER HERE**...\n",
    "As indicated above, overfitting occurs when our model becomes too sensitive to the noise in our particular training data, limiting its ability to make accurate predictions on out-of-sample data. This is an issue because there are very few instances in which it would be ideal to make predictions just on one particular sample, most likely, we would like to be able to predict beyond our sample with accuracy. Overfitting is a common issue in machine learning modeling, and we make sure to evaluate and safeguard against it when we're creating our models. The train-test-validation split helps us in this regard. As the name suggests, we divide our data firstly into two subsets - a training and testing set. We train our model on the training data. Then we split the training data again (leaving aside the testing data) into validation sets. We use this validation set in order to make adjustments to our model before we use the 'test' dataset, as we want to leave that untouched until our model is ready. Then we test our algorithm finally using the test set that we set aside."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e64e982",
   "metadata": {},
   "source": [
    "3. Discuss your analysis in 2-3 paragraphs. Discuss your findings and recommendations. Which counties or regions would you prioritize for the pilot program? Would your answers change based on whether you want to take into account certain features such as race, gender, or age composition in the county? How confident would you be deploying this sort of model in a real-world application – why or why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55dd703e",
   "metadata": {},
   "source": [
    "**YOUR ANSWER HERE**...\n",
    "At the outset of our exploration of the data, there seemed to be myriad features that correlated to diabetes. Though, it is important to note that some of these features seemed more related to our outcome of interest than others, of course, and, additionally, it also seemed like there might be some colinearity between features. It is hard to give suggestions to this policymaker interested in implementing a diabetes prevention pilot program for several reasons. Firstly, we do not know *what* specific intervention the policymaker intends to implement. Knowing what the intervention is in advance might change the way we approach the data. \n",
    "\n",
    "With this aside, the goal of this analysis was to observe and model relationships between features in this dataset and our outcome of interest, Diabetes Rates. We firstly did some preliminary exploration of the data by examining relationships between different features. After seeing preliminary indications that physical inactivity, race, and obesity were both correlated with diabetes rates, we looked at variation in these features by-region and by-county, as this is the level of analysis that the policymaker is interested in. These preliminary analyses suggested that diabetes rates may be higher in the Southeast and South - we could speculate that this might be owing to any number of cultural, environmental, infrastructural, or economic reasons such as typical hobbies, weather, access to healthcare or public transportation, the existence of food deserts, etc. We then turned to our models to better study the relationships between features and our outcome, diabetes rates.\n",
    "\n",
    "After finding and fitting our best possible model, we utilized a LASSO model to evaluate relevant features and their relationship to diabetes rates. In doing so, LASSO identified a small number of particularly important features - namely, physical inactivity, race (prevalence of white or indigenous people), and to a smaller extent, age. (See more below)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e911f030-d180-4fab-aa4c-8252caec9a31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Physical Inactivity Threshold: 0.2310405196180242\n",
      "white Threshold: 0.9501927157025192\n",
      "indigenous Threshold: 0.020071385271324023\n"
     ]
    }
   ],
   "source": [
    "#Create a threshold\n",
    "physical_inactivity_threshold = diabetes_counties['Physical_Inactivity_Number'].quantile(0.75)  # top 25%\n",
    "white_threshold = diabetes_counties['race_total population_one race_white'].quantile(0.75)  # top 23\n",
    "indigenous_threshold = diabetes_counties[\"race alone or in combination with one or more other races_total population_american indian and alaska native\"].quantile(0.75)\n",
    "print(f\"Physical Inactivity Threshold: {physical_inactivity_threshold}\")\n",
    "print(f\"white Threshold: {white_threshold}\")\n",
    "print(f\"indigenous Threshold: {indigenous_threshold}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6f5f26aa-24fe-4fb8-b754-1db1752426a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of DataFrame: (20, 90)\n"
     ]
    }
   ],
   "source": [
    "# Create a list of thresholds\n",
    "thresholds = {\n",
    "    'Physical_Inactivity_Number': physical_inactivity_threshold,\n",
    "    'race_total population_one race_white': white_threshold,\n",
    "    \"race alone or in combination with one or more other races_total population_american indian and alaska native\":indigenous_threshold,\n",
    "}\n",
    "\n",
    "# Create an empty list to store the targeted counties\n",
    "targeted_counties = []\n",
    "\n",
    "# Iterate through each row in the DataFrame\n",
    "for index, row in diabetes_counties.iterrows():\n",
    "    if all(row[col] > thresholds[col] for col in thresholds):\n",
    "        targeted_counties.append(row)\n",
    "\n",
    "# Convert the list of targeted counties back to a DataFrame\n",
    "targeted_counties_df = pd.DataFrame(targeted_counties)\n",
    "\n",
    "# Display the targeted counties\n",
    "#print(targeted_regions_df)\n",
    "print(\"Shape of DataFrame:\", targeted_counties_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "81111e18-e662-4189-ab7e-5b741ddef588",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3112, 90)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diabetes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8ec2983c-df68-4a02-aaf4-bfe148535113",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "115          Boone County\n",
      "177          Sharp County\n",
      "179          Stone County\n",
      "924      Greenwood County\n",
      "941           Linn County\n",
      "958        Osborne County\n",
      "970           Rush County\n",
      "1490        Benton County\n",
      "1500        Carter County\n",
      "1557        Oregon County\n",
      "1587         Stone County\n",
      "1593    Washington County\n",
      "1664          Burt County\n",
      "1713     Mcpherson County\n",
      "2005        Foster County\n",
      "2235       Sherman County\n",
      "2575          Polk County\n",
      "2639          Clay County\n",
      "2859       Daggett County\n",
      "3190      Washburn County\n",
      "Name: County, dtype: object\n"
     ]
    }
   ],
   "source": [
    "county_names = targeted_counties_df['County']\n",
    "print(county_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad9f78a6-e2d1-4f2d-8388-b9439e211217",
   "metadata": {},
   "source": [
    "**YOUR ANSWER 3 HERE Continued...**\n",
    "Mathematically, we can (and have) calculated counties to target with our pilot program. These are all the counties that meet the threshold of inclusion based on our most important features shown in our LASSO model. Although this is true, the implementation of any policy should be undergirded by a contextual understanding of each specific county. Just because our statistical model tells us that these are the counties we should target because they meet a statistical threshold, we still need to understand *why* this is the case. The answer to this question requires a contextual, and to some extent, historical understanding of these counties that the data simply cannot provide. Moreover, although LASSO has indicated that there are certain aforementioned important features in the data, we cannot be sure what unaccounted-for features might exist and we can wonder (especially given our far from perfect model performance) how those unobserved features might be influencing outcomes. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
