---
title: 'Project 6: Randomization and Matching'
output: pdf_document
---

# Introduction

In this project, you will explore the question of whether college education causally affects political participation. Specifically, you will use replication data from \href{https://papers.ssrn.com/sol3/papers.cfm?abstract_id=1409483}{Who Matches? Propensity Scores and Bias in the Causal Eï¬€ects of Education on Participation} by former Berkeley PhD students John Henderson and Sara Chatfield. Their paper is itself a replication study of \href{https://www.jstor.org/stable/10.1017/s0022381608080651}{Reconsidering the Effects of Education on Political Participation} by Cindy Kam and Carl Palmer. In their original 2008 study, Kam and Palmer argue that college education has no effect on later political participation, and use the propensity score matching to show that pre-college political activity drives selection into college and later political participation. Henderson and Chatfield in their 2011 paper argue that the use of the propensity score matching in this context is inappropriate because of the bias that arises from small changes in the choice of variables used to model the propensity score. They use \href{http://sekhon.berkeley.edu/papers/GenMatch.pdf}{genetic matching} (at that point a new method), which uses an approach similar to optimal matching to optimize Mahalanobis distance weights. Even with genetic matching, they find that balance remains elusive however, thus leaving open the question of whether education causes political participation.

You will use these data and debates to investigate the benefits and pitfalls associated with matching methods. Replication code for these papers is available online, but as you'll see, a lot has changed in the last decade or so of data science! Throughout the assignment, use tools we introduced in lab from the \href{https://www.tidyverse.org/}{tidyverse} and the \href{https://cran.r-project.org/web/packages/MatchIt/MatchIt.pdf}{MatchIt} packages. Specifically, try to use dplyr, tidyr, purrr, stringr, and ggplot instead of base R functions. While there are other matching software libraries available, MatchIt tends to be the most up to date and allows for consistent syntax.

# Data

The data is drawn from the \href{https://www.icpsr.umich.edu/web/ICPSR/studies/4023/datadocumentation#}{Youth-Parent Socialization Panel Study} which asked students and parents a variety of questions about their political participation. This survey was conducted in several waves. The first wave was in 1965 and established the baseline pre-treatment covariates. The treatment is whether the student attended college between 1965 and 1973 (the time when the next survey wave was administered). The outcome is an index that calculates the number of political activities the student engaged in after 1965. Specifically, the key variables in this study are:

\begin{itemize}
    \item \textbf{college}: Treatment of whether the student attended college or not. 1 if the student attended college between 1965 and 1973, 0 otherwise.
    \item \textbf{ppnscal}: Outcome variable measuring the number of political activities the student participated in. Additive combination of whether the student voted in 1972 or 1980 (student\_vote), attended a campaign rally or meeting (student\_meeting), wore a campaign button (student\_button), donated money to a campaign (student\_money), communicated with an elected official (student\_communicate), attended a demonstration or protest (student\_demonstrate), was involved with a local community event (student\_community), or some other political participation (student\_other)
\end{itemize}

Otherwise, we also have covariates measured for survey responses to various questions about political attitudes. We have covariates measured for the students in the baseline year, covariates for their parents in the baseline year, and covariates from follow-up surveys. \textbf{Be careful here}. In general, post-treatment covariates will be clear from the name (i.e. student\_1973Married indicates whether the student was married in the 1973 survey). Be mindful that the baseline covariates were all measured in 1965, the treatment occurred between 1965 and 1973, and the outcomes are from 1973 and beyond. We will distribute the Appendix from Henderson and Chatfield that describes the covariates they used, but please reach out with any questions if you have questions about what a particular variable means.

```{r}
# Load tidyverse and MatchIt
# Feel free to load other libraries as you wish
library(tidyverse)
library(MatchIt)

# set seed
# ----------
set.seed(13)

# libaries
# ----------
xfun::pkg_attach2(c("tidyverse", 
                    "here", 
                    "MatchIt",   # for matching
                    "optmatch",  # for matching
                    "cobalt",   # for matching assessment
                    "knitr",       # for kniting together tables 
                    "kableExtra")) # for styling

# set filepath with here()
# ----------
here()

# prevent scientific notation
# ----------
options(scipen = 999)

# Load ypsps data
ypsps <- read_csv('data/ypsps.csv')
head(ypsps)
spec(ypsps)
```

# Randomization

Matching is usually used in observational studies to to approximate random assignment to treatment. But could it be useful even in randomized studies? To explore the question do the following:

\begin{enumerate}
    \item Generate a vector that randomly assigns each unit to either treatment or control
    \item Choose a baseline covariate (for either the student or parent). A binary covariate is probably best for this exercise.
    \item Visualize the distribution of the covariate by treatment/control condition. Are treatment and control balanced on this covariate?
    \item Simulate the first 3 steps 10,000 times and visualize the distribution of treatment/control balance across the simulations.
\end{enumerate}


```{r}
# Load necessary libraries
library(dplyr)
library(ggplot2)
library(tidyr)

# Generate a vector that randomly assigns each unit to treatment/control
# Create a variable that is the number of units in the dataset
n_units <- nrow(ypsps)

# Generate a random (hypothetical) treatment assignment (0 = control, 1 = treatment)
set.seed(123)  # Set seed for reproducibility
treatment_assignment <- sample(c(0, 1), size = n_units, replace = TRUE)

# Add the treatment assignment to the dataframe
df_random <- ypsps
df_random$hypotreatment <- treatment_assignment

# Compare randomly assigned (hypotreatment) to actual treatment (college) on outcome (student_ppnscal)
# Calculate summary statistics (mean ppnscal) for each treatment group
summary_treatment <- df_random %>%
  group_by(college, hypotreatment) %>%
  summarize(mean_ppnscal = mean(student_ppnscal, na.rm = TRUE),
            sd_ppnscal = sd(student_ppnscal, na.rm = TRUE),
            n = n())

# View the summary statistics
print(summary_treatment)

```
Okay, so I think basically this shows the potential effects of randomizing college attendance (if that was something we could do)? Group 1, actualtreatment 0 and hypotreatment 0, represent those who both did not actually attend college and were also randomly assigned not to attend college in our simulation. Group 2, actualtreatment 0 and hypotreatment 1, represent those who did not actually attend college but were assigned to in our simulation. Group 3, actualtreatment 1 and hypotreatment 0, represent those who did actually go to college but were randomly assigned to the non-college group. And Group 4, actualtreatment 1 and hypotreatment 1, are those who both did actually go to college and were also randomly assigned to go to college. 

```{r}
# Choose a baseline covariate (use dplyr for this)
# Identify binary variables (variables with exactly 2 unique values)
binary_vars <- sapply(df_random, function(x) length(unique(x)) == 2)

# View the names of the binary variables
binary_var_names <- names(binary_vars[binary_vars])
binary_var_names
```
For my covariate, I am going to use "parent_Persuade," which, according to the codebook I found online, is a y/n response to the question "Did you talk to any people and try to show them why they should vote one way or the other?" I think this is specifically for the 1964 election. My rationale is that maybe this has an impact on individuals if, growing up, their parents are politically active enough to try and persuade others, and maybe moving out of the home for the first time in college changes students' worldview. It might be interesting to see whether this varies by race, class, etc. I wanted to do a cooler variable like parent_Govt4All, but they're categorical.
```{r}
table(df_random$parent_Persuade)
```
Let's also say it at the top here in case I forget to put it in the discussion questions later - the treatment variable is only college attendance between 1965-1973, so doesn't include some possible non-traditional students. Maybe they started college in 1973 and voted in the 1980 election, this doesn't account for that.
```{r}
# Visualize the distribution by treatment/control (ggplot)
# Create a summarized dataset
tidy_df <- df_random %>%
  # Group by randomized treatment (hypotreatment)
  group_by(hypotreatment) %>%
  # Summarize the data by calculating the count of parent_Persuade responses
  summarize(
    total_count = n(),  # Total number of observations
    persuade_count = sum(parent_Persuade == 1, na.rm = TRUE),  # Count of "Yes" for parent_Persuade
    no_persuade_count = sum(parent_Persuade == 0, na.rm = TRUE),  # Count of "No" for parent_Persuade
    persuade_prop = persuade_count / total_count,  # Proportion of "Yes"
    no_persuade_prop = no_persuade_count / total_count  # Proportion of "No"
  )

# View the summarized data 
print(tidy_df)

# Reshape the data to long format for plotting proportions
tidy_df_long <- tidy_df %>%
  pivot_longer(cols = c("persuade_prop", "no_persuade_prop"), 
               names_to = "parent_persuade_status", 
               values_to = "proportion") %>%
  mutate(parent_persuade_status = recode(parent_persuade_status, 
                                        "persuade_prop" = "Yes", 
                                        "no_persuade_prop" = "No"))

# Create the plot to visualize both 'Yes' and 'No' proportions of 'parent_Persuade' by 'college' (treatment/control)
ggplot(tidy_df_long, aes(x = factor(hypotreatment), y = proportion, fill = parent_persuade_status)) +
  geom_bar(stat = "identity", position = "dodge") +  # Position bars side by side for Yes and No
  labs(
    x = "Randomized Treatment (0 = Control, 1 = Treatment)", 
    y = "Parental Encourage Proportion", 
    fill = "Parental Encouragement to Vote (1964)", 
    title = "Distribution of Parental Encouragement to Vote by Randomized College Attendance"
  ) +
  scale_fill_manual(values = c("brown1", "chartreuse2")) +  
  theme_minimal()  # Use a clean theme
```
From this plot we can see that our randomization worked to (more or less) balance the distribution of this covariate across our treatment and control groups.
For fun, let's compare this to the balance of the distribution for actual treatment (college), not hypothetical treatment (hypotreatment) like we just did.
```{r}
# Create a summarized dataset
tidy_df <- df_random %>%
  # Group by randomized treatment (hypotreatment)
  group_by(college) %>%
  # Summarize the data by calculating the count of parent_Persuade responses
  summarize(
    total_count = n(),  # Total number of observations
    persuade_count = sum(parent_Persuade == 1, na.rm = TRUE),  # Count of "Yes" for parent_Persuade
    no_persuade_count = sum(parent_Persuade == 0, na.rm = TRUE),  # Count of "No" for parent_Persuade
    persuade_prop = persuade_count / total_count,  # Proportion of "Yes"
    no_persuade_prop = no_persuade_count / total_count  # Proportion of "No"
  )

# View the summarized data 
print(tidy_df)

# Reshape the data to long format for plotting proportions
tidy_df_long <- tidy_df %>%
  pivot_longer(cols = c("persuade_prop", "no_persuade_prop"), 
               names_to = "parent_persuade_status", 
               values_to = "proportion") %>%
  mutate(parent_persuade_status = recode(parent_persuade_status, 
                                        "persuade_prop" = "Yes", 
                                        "no_persuade_prop" = "No"))

# Create the plot to visualize both 'Yes' and 'No' proportions of 'parent_Persuade' by 'college' (treatment/control)
ggplot(tidy_df_long, aes(x = factor(college), y = proportion, fill = parent_persuade_status)) +
  geom_bar(stat = "identity", position = "dodge") +  # Position bars side by side for Yes and No
  labs(
    x = "Actual College (0 = No, 1 = Yes)", 
    y = "Parent Encourage Proportion", 
    fill = "Parental Encouragement to Vote (1964)", 
    title = "Distribution of Parental Encouragement to Vote by Actual College Attendance"
  ) +
  scale_fill_manual(values = c("brown1", "chartreuse2")) +  
  theme_minimal()  # Use a clean theme
```


Yeah, way less balance here. This may or may not be surprising, but the proportion of children with parents who encouraged others to vote is higher amongst those children who went to college versus those who did not.

```{r}
# Simulate this 20-30 times (monte carlo simulation - see R Refresher for a hint)
# Define a function to run the Monte Carlo simulation
run_monte_carlo <- function(ypsps, n_simulations = 30) {
  
  # Initialize an empty list to store results
  simulation_results <- list()
  
  # Run the simulation n_simulations times
  for (i in 1:n_simulations) {
    
    # Generate a random (hypothetical) treatment assignment (0 = control, 1 = treatment)
    set.seed(123 + i)  # Ensure different randomization by changing seed each time
    treatment_assignment <- sample(c(0, 1), size = nrow(ypsps), replace = TRUE)
    
    # Add the treatment assignment to the dataframe
    df_random <- ypsps
    df_random$hypotreatment <- treatment_assignment
    
    # Summarize the data by treatment group
    tidy_df <- df_random %>%
      group_by(hypotreatment) %>%
      summarize(
        total_count = n(),  
        persuade_count = sum(parent_Persuade == 1, na.rm = TRUE),  
        no_persuade_count = sum(parent_Persuade == 0, na.rm = TRUE),  
        persuade_prop = persuade_count / total_count,  
        no_persuade_prop = no_persuade_count / total_count  
      )
    
    # Reshape to long format for plotting
    tidy_df_long <- tidy_df %>%
      pivot_longer(cols = c("persuade_prop", "no_persuade_prop"), 
                   names_to = "parent_persuade_status", 
                   values_to = "proportion") %>%
      mutate(parent_persuade_status = recode(parent_persuade_status, 
                                            "persuade_prop" = "Yes", 
                                            "no_persuade_prop" = "No"))
    
    # Add the results of the current simulation to the list
    simulation_results[[i]] <- tidy_df_long
  }
  
  # Combine all results into one data frame
  combined_results <- bind_rows(simulation_results)
  
  return(combined_results)
}

# Run the Monte Carlo simulation with 30 repetitions
combined_results <- run_monte_carlo(ypsps, n_simulations = 30)

# Create the plot to visualize the distribution of proportions for all simulations
ggplot(combined_results, aes(x = factor(hypotreatment), y = proportion, fill = parent_persuade_status)) +
  geom_bar(stat = "identity", position = "dodge") +  # Position bars side by side for Yes and No
  labs(
    x = "Randomized Treatment (0 = Control, 1 = Treatment)", 
    y = "Parental Encourage Proportion", 
    fill = "Parental Encouragement to Vote (1964)", 
    title = "Distribution of parent_Persuade by Monte Carlo Randomization"
  ) +
  scale_fill_manual(values = c("brown1", "chartreuse2")) +  # Customize bar colors for Yes/No
  theme_minimal() +  # Use a clean theme
  theme(
    legend.position = "top",  # Position legend at the top
    axis.title.x = element_text(size = 12, face = "bold"),  # Customize axis title for X-axis
    axis.title.y = element_text(size = 12, face = "bold"),  # Customize axis title for Y-axis
    plot.title = element_text(size = 14, face = "bold", hjust = 0.5)  # Customize plot title
  )
```

## Questions
\begin{enumerate}
    \item \textbf{What do you see across your simulations? Why does independence of treatment assignment and baseline covariates not guarantee balance of treatment assignment and baseline covariates?}
\end{enumerate}

Your Answer:
I mean, unless I'm interpreting this wrong, it actually looks like the balance of this covariate across treatment/control was pretty good both for the one original random assignment and the Monte Carlo simulation. However, I think theoretically, the independence of treatment assignment and baseline covariates doesn't guarantee balance of treatment assignment and baseline covariates because we are only running so many random samples. The more we do it and average it over those samples, I think we should get more balance. 

# Propensity Score Matching

## One Model
Select covariates that you think best represent the "true" model predicting whether a student chooses to attend college, and estimate a propensity score model to calculate the Average Treatment Effect on the Treated (ATT). Plot the balance of the top 10 (or fewer if you select fewer covariates). Report the balance of the p-scores across both the treatment and control groups, and using a threshold of standardized mean difference of p-score $\leq .1$, report the number of covariates that meet that balance threshold.
I mean, I'm assuming that a lot of the most useful variables predicting whether a student goes to college are those related to their parents. I'm still a little confused because I know that post-treatment covariates typically will will include 1973 in the label, but then there's something like "student_PubAff" which is the number of public affairs classes the student has taken. But it's not clear if this is a baseline covariate (how many public affairs classes can one possibly take in high school?) or if it's a post-treatment covariate that just doesn't say 1973. In general, the confusingness of the codebook (it doesn't have the variable names!) from the study in question means that I theorize I'll leave out some things from the model that are important. 


```{r}
# Select covariates that represent the "true" model for selection, fit model
#All of these are baseline covariates, I think (hope?) I looked at the codebook from the study but as mentioned above, it's not super clear.
#student_LifeWish - Does the respondent's life go as they wish (may be indicator of privilege)
#student_FPlans - Does the respondent finish their plans (may be indicator of motivation)
#student_MChange - Is the respondent's mind easily changed (like the Brand and Xie paper, could be indicator of social expectation to go to college)
#student_StrOpinion - Does the respondent have strong opinions (maybe the opposite of student_MChange)
#student_GPA 
#student_NextSch - plans for next school year, self-evident but could think about Brand and Xie paper
#student_Knowledge - "Youth knowledge index" whatever that means
#student_SchOfficer - Officer in a school organization (may be indicator of motivation)
#student_Phone - Does the respondent have a phone at home (may be indicator of privilege)
#student_Gen - Gender
#student_Race - Race
#parent_LifeWish
#parent_StrOpinion
#parent_MChange - Same questions as given to the children, but I'd imagine the reverse impact - if a parent scores high on "parent_StrOpinion" and a child scores high on "student_MChange," then the parent's agenda might dictate whether the child goes to college.
#parent_FInc - (not sure what this one is but assuming it's related to income)
#parent_HHInc - Household income
#parent_OwnHome - Homeownership
#parent_Employ - Employed
#parent_EducHH - Level of education
#parent_EducW (not sure what this one is but assuming it's related to income)
#parent_Race
#parent_Gen
#I wanted to create a vector with all of these 'true' estimate covariates but it kept breaking things so I just keep copy-pasting them it's very clunky
#Propensity score model
# Fit a logistic regression model to estimate the propensity scores with all selected covariates that represent the "true" model for college attendance
#Propensity score model
 model_ps <- glm(college ~ student_LifeWish + student_FPlans + student_MChange + 
                 student_StrOpinion + student_GPA + student_NextSch + student_Knowledge + 
                 student_SchOfficer + student_Phone + student_Gen + student_Race + 
                 parent_LifeWish + parent_StrOpinion + parent_MChange + parent_FInc + 
                 parent_HHInc + parent_OwnHome + parent_Employ + parent_EducHH + 
                 parent_EducW + parent_Race + parent_Gen,
                family = binomial(), data = ypsps)

#  Add the propensity scores to the original dataset ypsps
 ypsps <- ypsps %>%
 mutate(prop_score = predict(model_ps, type = "response"))
# Check to confirm propensity scores have been added
head(ypsps$prop_score)
```

```{r}
#Estimate ATT
library(MatchIt)

match_model <- matchit(
  college ~ student_LifeWish + student_FPlans + student_MChange +
    student_StrOpinion + student_GPA + student_NextSch + student_Knowledge +
    student_SchOfficer + student_Phone + student_Gen + student_Race +
    parent_LifeWish + parent_StrOpinion + parent_MChange + parent_FInc +
    parent_HHInc + parent_OwnHome + parent_Employ + parent_EducHH +
    parent_EducW + parent_Race + parent_Gen,
  method = "full",  # Method for full matching
  estimand = "ATT",
  data = ypsps)
#get the matched dataset
matched_data <- match.data(match_model)

# Fit a linear regression model with weights to estimate ATT
lm_model <- lm(student_ppnscal ~ college + student_LifeWish + student_FPlans + student_MChange +
                student_StrOpinion + student_GPA + student_NextSch + student_Knowledge +
                student_SchOfficer + student_Phone + student_Gen + student_Race +
                parent_LifeWish + parent_StrOpinion + parent_MChange + parent_FInc +
                parent_HHInc + parent_OwnHome + parent_Employ + parent_EducHH +
                parent_EducW + parent_Race + parent_Gen, 
               data = matched_data, 
               weights = weights)  # Use weights from matching
ATT_estimate <- summary(lm_model)$coefficients["college", "Estimate"]
print(paste("The estimated Average Treatment Effect on the Treated (ATT) is:", round(ATT_estimate, 3)))
```
I think this shows that on average, students who attended college participate in political activities at a rate that is 0.85 (units) greater than those who did not attend college. Or, is it that students who attended college are 0.85x more likely to participate in political activities?

```{r}
# Plot the balance for the top 10 covariates (the PDF doesn't say 10 specifically but the notebook does so I'm running with it)
# Extract standardized mean differences (SMD) for the covariates before and after matching 
library(purrr)

# Get the SMDs using pluck
balance_matched <- summary(match_model) %>% 
  pluck("sum.matched") %>% 
  as.data.frame()

# Add variable names as a column
balance_matched <- balance_matched %>%
  mutate(Variable = rownames(.))

# View SMDs
print(balance_matched$`Std. Mean Diff.`)

# na_vars <- rownames(balance_matched)[is.na(balance_matched$`Var. Ratio`)]
# print(na_vars)
```

```{r}
#Actual plot of balance for top 10 covariates
# Determine top 10 covariates by absolute SMD
top_10_smd <- balance_matched %>%
  arrange(abs(`Std. Mean Diff.`)) %>%   # ascending order = most balanced first
  select(Variable, `Std. Mean Diff.`) %>%
  slice(1:10)
library(ggplot2)
library(dplyr)

# Plot
ggplot(top_10_smd, aes(x = reorder(Variable, abs(`Std. Mean Diff.`)), 
                       y = `Std. Mean Diff.`)) +
  geom_col(fill = "steelblue") +
  geom_hline(yintercept = c(-0.1, 0.1), linetype = "dashed", color = "red") +
  coord_flip() +
  labs(title = "Top 10 Covariates by Standardized Mean Difference (Post-Matching)",
       x = "Covariate",
       y = "Standardized Mean Difference") +
  theme_minimal()
print(top_10_smd)
```
Great, so it looks like all my top 10 covariates have SMDs within the .1 threshold, so it seems like maybe I picked covariates that are helpful in thinking about the model of treatment (college attendance).The positive SMDs indicate that the values are higher for those in the treatment group than the control, I think, whereas negative SMDs indicate the opposite. So the treatment group has higher GPAs and their parents scored higher on the measure parent_MChange, which is whether their mind is easily changed, which is interesting, perhaps indicating sucepitiblity to social presure.

```{r}
# Report balance of p-scores across both treatment and control groups
library(survey)

# Create a survey design object with matching weights (apparently I need to do this because matchit uses weights? and I don't want to ignore them because it'll bias my SMD)
design <- svydesign(ids = ~1, data = matched_data, weights = ~weights)

# Calculate weighted means for treated and control
mean_treated <- mean(matched_data$distance[matched_data$college == 1])  # treated are all weighted = 1
mean_control <- svymean(~distance, subset(design, college == 0))[1]

# Calculate weighted variance for both groups
var_treated <- var(matched_data$distance[matched_data$college == 1])
var_control <- svyvar(~distance, subset(design, college == 0))[1]

# Pooled SD
sd_pooled <- sqrt((var_treated + var_control) / 2)

# Compute SMD
smd_pscore_matched <- (mean_treated - mean_control) / sd_pooled
print(smd_pscore_matched)
```
Okay, I think the distance is the p-score, as in the SMD of p-scores between our treatment and control group. This is well below the 0.1 threshold!

```{r}
# Count how many covariates have an absolute SMD â‰¤ 0.1
num_balanced_covariates <- balance_matched %>%
  filter(abs(`Std. Mean Diff.`) <= 0.1) %>%
  nrow()

print(num_balanced_covariates)
```
10 out of 23 isn't so bad I guess...


## Simulations

Henderson/Chatfield argue that an improperly specified propensity score model can actually \textit{increase} the bias of the estimate. To demonstrate this, they simulate 800,000 different propensity score models by choosing different permutations of covariates. To investigate their claim, do the following:

\begin{itemize}
    \item Using as many simulations as is feasible (at least 10,000 should be ok, more is better!), randomly select the number of and the choice of covariates for the propensity score model.
    \item For each run, store the ATT, the proportion of covariates that meet the standardized mean difference $\leq .1$ threshold, and the mean percent improvement in the standardized mean difference. You may also wish to store the entire models in a list and extract the relevant attributes as necessary.
    \item Plot all of the ATTs against all of the balanced covariate proportions. You may randomly sample or use other techniques like transparency if you run into overplotting problems. Alternatively, you may use plots other than scatterplots, so long as you explore the relationship between ATT and the proportion of covariates that meet the balance threshold.
    \item Finally choose 10 random models and plot their covariate balance plots (you may want to use a library like \href{https://cran.r-project.org/web/packages/gridExtra/index.html}{gridExtra} to arrange these)
\end{itemize}

\textbf{Note: There are lots of post-treatment covariates in this dataset (about 50!)! You need to be careful not to include these in the pre-treatment balancing. Many of you are probably used to selecting or dropping columns manually, or positionally. However, you may not always have a convenient arrangement of columns, nor is it fun to type out 50 different column names. Instead see if you can use dplyr 1.0.0 functions to programatically drop post-treatment variables (\href{https://www.tidyverse.org/blog/2020/03/dplyr-1-0-0-select-rename-relocate/}{here} is a useful tutorial).}

```{r}
colnames(ypsps)
```

```{r}
set.seed(123)
# Define the covariates to exclude
exclude_columns <- c("student_ppnscal", "college", "interviewid", "prop_score")
# Create covariate_names by excluding those from ypsps along with the post-treatment covariates that contain years
covariate_names <- ypsps %>%
  select(-all_of(exclude_columns)) %>%
  select(-contains("1973"), -contains("1982")) %>%
  names()
# Simulation parameters
n_simulations <- 30  # Number of simulations
ps_results <- list()    # List to store results

# Function to calculate standardized mean difference (SMD)
calculate_smd <- function(before, after) {
  (mean(before) - mean(after)) / sqrt((sd(before)^2 + sd(after)^2) / 2)
}

# Simulate random selection of features 20-30 times
# Fit  models and save ATTs, proportion of balanced covariates, and mean percent balance improvement
# Simulation loop
for (i in 1:n_simulations) {

  # Randomly select covariates
  num_covariates <- sample(5:10, 1)
  selected_covariates <- sample(covariate_names, num_covariates)

  # Select covariates + treatment and outcome
  covariate_data <- ypsps %>%
    select(all_of(selected_covariates), college, student_ppnscal)  # Selecting covariates

  # Check for any NAs and remove rows with NAs before fitting model
  if (sum(is.na(covariate_data)) > 0) {
    covariate_data <- covariate_data %>% drop_na()  # Remove rows with missing values
  }

  # Fit propensity score model
  ps_model <- glm(college ~ ., data = covariate_data %>% select(-student_ppnscal), family = binomial)
  covariate_data$prop_score <- predict(ps_model, type = "response")

  # Perform matching using the 'nearest' method and prop_score
  match_model <- matchit(
  college ~ .,
  data = covariate_data %>% select(all_of(selected_covariates), college),
  method = "nearest",
  distance = covariate_data$prop_score,
  replace = TRUE  # Allow controls to be matched multiple times (I didn't like that not all treated groups were getting a match)
 )
  matched_data <- match.data(match_model)

  # Skip if no matches or only one group remains
  if (nrow(matched_data) == 0 || length(unique(matched_data$college)) < 2) next

  # Add outcome variable back to matched data
  matched_data$student_ppnscal <- covariate_data$student_ppnscal[as.numeric(rownames(matched_data))]

  # Estimate ATT (Average Treatment Effect on the Treated)
  att <- mean(matched_data$student_ppnscal[matched_data$college == 1], na.rm = TRUE) -
         mean(matched_data$student_ppnscal[matched_data$college == 0], na.rm = TRUE)

  # Calculate SMDs before matching
  smd_before <- sapply(selected_covariates, function(var) {
    calculate_smd(
      covariate_data[[var]][covariate_data$college == 1],
      covariate_data[[var]][covariate_data$college == 0]
    )
  })

  # Calculate SMDs after matching
  smd_after <- sapply(selected_covariates, function(var) {
    calculate_smd(
      matched_data[[var]][matched_data$college == 1],
      matched_data[[var]][matched_data$college == 0]
    )
  })

  # Proportion of covariates balanced (SMD â‰¤ 0.1)
  prop_balanced <- mean(smd_after <= 0.1)
  # Mean improvement in SMD
  mean_smd_improvement <- mean(smd_before - smd_after)

  # Store results for each simulation iteration
  ps_results[[i]] <- list(
  ATT = att,
  prop_balanced = prop_balanced,
  mean_smd_improvement = mean_smd_improvement,
  match_model = match_model,
  covariate_data = covariate_data,
  selected_covariates = selected_covariates,
  smd_before = smd_before,
  smd_after = smd_after
)
}

# Summarize simulation results
ps_simulation_df <- do.call(rbind, lapply(ps_results, function(res) {
  data.frame(
    ATT = res$ATT,
    prop_balanced = res$prop_balanced,
    mean_smd_improvement = res$mean_smd_improvement
  )
}))

# Display summary statistics
summary(ps_simulation_df)
```
The mean of my ATT is .01586, so definitely not great. There's also variability from negative numbers to slightly positive. In general, perhaps picking covariates randomly is not the best way to go!

```{r}
#Plot all the ATTs against all of the balanced covariate proportions
# Create scatter plot
ggplot(ps_simulation_df, aes(x = prop_balanced, y = ATT)) +
  geom_point(alpha = 0.5) +  # Alpha adds transparency to avoid overplotting
  labs(
    title = "ATT vs Proportion of Covariates Balanced",
    x = "Proportion of Covariates Balanced (SMD <= 0.1)",
    y = "Average Treatment Effect on the Treated (ATT)"
  ) +
  theme_minimal()
```
So, my interpretation of this plot is that, in general over the simulations, the ATT is fairly low (I guess I don't know what the best practice value is for ATT, though). At the same time, as the proportion of balance covariates increases (measured by SMDs less than 0.1), the ATT also increases generally. I guess that makes sense, because if there's imbalanced covariates, then that might complicate our ATT.


```{r}
library(ggplot2)

# Choose 5 random simulation results
sample_results <- sample(ps_results, 5)

# Function to plot balance for a given match model
plot_balance <- function(res, idx) {
  selected_covariates <- res$selected_covariates
  smd_before <- res$smd_before
  smd_after <- res$smd_after

  balance_data <- data.frame(
    Covariate = selected_covariates,
    SMD_Before = smd_before,
    SMD_After = smd_after
  )

  ggplot(balance_data, aes(x = Covariate)) + 
    geom_bar(aes(y = SMD_Before, fill = "Before"), stat = "identity", alpha = 0.6) +
    geom_bar(aes(y = SMD_After, fill = "After"), stat = "identity", alpha = 0.6) +
    coord_flip() +
    scale_fill_manual(values = c("Before" = "red", "After" = "green")) +
    labs(title = paste("Covariate Balance (Run", idx, ")"),
         x = "Covariate", y = "SMD") +
    theme_minimal() +
    theme(axis.text.y = element_text(size = 7),
          legend.title = element_blank(),
          legend.position = "top")
}

# Loop through each sample result and plot them individually
lapply(1:5, function(i) {
  plot_balance(sample_results[[i]], i)  
})

```




## Questions

\begin{enumerate}
    \item \textbf{How many simulations resulted in models with a higher proportion of balanced covariates? Do you have any concerns about this?}
```{r}    
  # Count how many simulations had above average balance
above_average_balance <- sum(simulation_df$prop_balanced > mean(simulation_df$prop_balanced))
print(paste("The number of models with an above average balance is:", above_average_balance))
```
    Your Answer: I guess I don't fully understand the question - higher proportion with respect to what? I'm going to assume it's higher proportion than average. So 15 of the 30 simulations had a higher proportion of balanced covariates which I think is good right? 
    \item \textbf{Analyze the distribution of the ATTs. Do you have any concerns about this distribution?}
    Your Answer: Addressed above 
    \item \textbf{Do your 5 randomly chosen covariate balance plots produce similar numbers on the same covariates? Is it a concern if they do not?}
    Your Answer: I think I might have broken my brain because doesn't the randomization of covariates and number of covariates mean that I don't necessarily have the same covariates present across all 5 models, as is the case here? I may have misunderstood the task. But in general, I don't think it's necessarily an issue because, again, this is random selection of our covariates. 
\end{enumerate}

# Matching Algorithm of Your Choice

## Simulate Alternative Model

Henderson/Chatfield propose using genetic matching to learn the best weights for Mahalanobis distance matching. Choose a matching algorithm other than the propensity score (you may use genetic matching if you wish, but it is also fine to use the greedy or optimal algorithms we covered in lab instead). Repeat the same steps as specified in Section 4.2 and answer the following questions:


```{r}
# Remove post-treatment covariates
# Define the covariates to exclude
exclude_columns <- c("student_ppnscal", "college", "interviewid", "prop_score")
# Create covariate_names by excluding those from ypsps along with the post-treatment covariates that contain years
covariate_pool_maha <- ypsps %>%
  select(-all_of(exclude_columns)) %>%
  select(-contains("1973"), -contains("1982")) %>%
  names()

set.seed(456)  # Different seed from 4.2

# Simulation parameters
n_maha_simulations <- 30  # Number of simulations
maha_results <- list()    # List to store results

# Function to calculate standardized mean difference (SMD)
calculate_smd <- function(before, after) {
  (mean(before) - mean(after)) / sqrt((sd(before)^2 + sd(after)^2) / 2)
}

# Simulate random selection of features 20-30 times
# Fit models and save ATTs, proportion of balanced covariates, and mean percent balance improvement
# Simulation loop
for (i in 1:n_maha_simulations) {
  
  # Randomly select covariates
  num_covs_maha <- sample(5:10, 1)
  selected_covs_maha <- sample(covariate_pool_maha, num_covs_maha)
  
  # Select covariates + treatment and outcome
  data_maha <- ypsps %>%
    select(all_of(selected_covs_maha), college, student_ppnscal) %>%
    drop_na()  # Check for any NAs and remove rows with NAs before fitting model
  
  # Perform matching using the Mahalanobis method which uses distance of covariates, not propensity score to match
  match_maha <- matchit(
    college ~ .,
    data = data_maha %>% select(all_of(selected_covs_maha), college),
    method = "nearest",
    distance = "mahalanobis", 
    replace = TRUE  # Allow controls to be matched multiple times (I didn't like that not all treated groups were getting a match)
  )
  
  matched_maha <- match.data(match_maha)

  # Skip if no matches or only one group remains
  if (nrow(matched_maha) == 0 || length(unique(matched_maha$college)) < 2) next
  
  # Add outcome variable back to matched data
  matched_maha$student_ppnscal <- data_maha$student_ppnscal[as.numeric(rownames(matched_maha))]
  
  # Estimate ATT (Average Treatment Effect on the Treated)
  att_maha <- mean(matched_maha$student_ppnscal[matched_maha$college == 1], na.rm = TRUE) -
              mean(matched_maha$student_ppnscal[matched_maha$college == 0], na.rm = TRUE)
  
  # Calculate SMDs before matching
  smd_before_maha <- sapply(selected_covs_maha, function(var) {
    calculate_smd(
      data_maha[[var]][data_maha$college == 1],
      data_maha[[var]][data_maha$college == 0]
    )
  })
  
  # Calculate SMDs after matching
  smd_after_maha <- sapply(selected_covs_maha, function(var) {
    calculate_smd(
      matched_maha[[var]][matched_maha$college == 1],
      matched_maha[[var]][matched_maha$college == 0]
    )
  })
  
  # Proportion of covariates balanced (SMD â‰¤ 0.1)
  prop_balanced_maha <- mean(smd_after_maha <= 0.1)
  
  # Mean improvement in SMD
  mean_smd_improve_maha <- mean(smd_before_maha - smd_after_maha)
  
  # Store results for each simulation iteration
  maha_results[[i]] <- list(
    ATT = att_maha,
    prop_balanced = prop_balanced_maha,
    mean_smd_improvement = mean_smd_improve_maha,
    match_model = match_maha,
    covariate_data = data_maha,
    selected_covariates = selected_covs_maha,
    smd_before = smd_before_maha,
    smd_after = smd_after_maha
  )
}

# Summarize Mahalanobis results
maha_df <- do.call(rbind, lapply(maha_results, function(res) {
  data.frame(
    ATT = res$ATT,
    prop_balanced = res$prop_balanced,
    mean_smd_improvement = res$mean_smd_improvement
  )
}))

# Display summary statistics
summary(maha_df)

```


```{r}
# 10 random covariate balance plots (hint try gridExtra)
# Note: ggplot objects are finnicky so ask for help if you're struggling to automatically create them; consider using functions!
library(ggplot2)

# Choose 10 random Mahalanobis matching covariate balance plots
sample_maha_results <- sample(maha_results, 10)

# Function to plot balance for a given Mahalanobis match model
plot_maha_balance <- function(res, idx) {
  selected_covariates <- res$selected_covariates
  smd_before <- res$smd_before
  smd_after <- res$smd_after

  balance_data <- data.frame(
    Covariate = selected_covariates,
    SMD_Before = smd_before,
    SMD_After = smd_after
  )

  ggplot(balance_data, aes(x = Covariate)) +
    geom_bar(aes(y = SMD_Before, fill = "Before"), stat = "identity", alpha = 0.6) +
    geom_bar(aes(y = SMD_After, fill = "After"), stat = "identity", alpha = 0.6) +
    coord_flip() +
    scale_fill_manual(values = c("Before" = "red", "After" = "green")) +
    labs(title = paste("Mahalanobis Covariate Balance (Run", idx, ")"),
         x = "Covariate", y = "SMD") +
    theme_minimal() +
    theme(axis.text.y = element_text(size = 7),
          legend.title = element_blank(),
          legend.position = "top")
}

# Loop through each sample Mahalanobis result and plot them individually
lapply(1:10, function(i) {
  plot_maha_balance(sample_maha_results[[i]], i)
})

```

```{r}
# Visualization for distributions of percent improvement comparing propensity model to Mahalanobis
# Extracting the percent improvement in balance (mean_smd_improvement) from both results
ps_improvement <- sapply(ps_results, function(res) res$mean_smd_improvement)
maha_improvement <- sapply(maha_results, function(res) res$mean_smd_improvement)

# Creating a data frame for plotting
comparison_data <- data.frame(
  Improvement = c(ps_improvement, maha_improvement),
  Method = rep(c("Propensity Score Matching", "Mahalanobis Matching"), each = length(ps_improvement))
)

# Plotting the distribution of percent improvement in balance for both methods
ggplot(comparison_data, aes(x = Improvement, fill = Method)) +
  geom_histogram(position = "dodge", bins = 15, alpha = 0.6) +
  scale_fill_manual(values = c("Propensity Score Matching" = "lightblue", "Mahalanobis Matching" = "lightgreen")) +
  labs(title = "Comparison of Percent Improvement in Covariate Balance",
       x = "Percent Improvement in Balance",
       y = "Frequency") +
  theme_minimal() +
  theme(axis.title.x = element_text(size = 12),
        axis.title.y = element_text(size = 12),
        axis.text.x = element_text(size = 10),
        axis.text.y = element_text(size = 10),
        legend.position = "top")
```

```{r}
#Create dataframe for visualizing different matching method metrics
# Add a column to indicate the matching method in both data frames
ps_simulation_df$Method <- "Propensity Score Matching"
maha_df$Method <- "Mahalanobis Matching"

# Combine the two data frames into one for comparison
comparison_df <- rbind(ps_simulation_df, maha_df)

# Summarize the results by method
summary_comparison <- data.frame(
  Method = c("Mahalanobis Matching", "Propensity Score Matching"),
  Mean_ATT = c(-0.03780403, 0.01586265),
  SD_ATT = c(0.1848872, 0.1356611),
  Mean_Prop_Balanced = c(0.6139286, 0.4788360),
  SD_Prop_Balanced = c(0.1893790, 0.1559769),
  Mean_SMD_Improvement = c(0.09140875, 0.05108180),
  SD_SMD_Improvement = c(0.10367009, 0.03542772)
)

# View the summary comparison table
print(summary_comparison)
```


## Questions

\begin{enumerate}
    \item \textbf{Does your alternative matching method have more runs with higher proportions of balanced covariates?}
     Your Answer: No, it doesn't. While I don't know much about the Mahalanobis Matching, the distinction is between matching on distance between values of covariates vs. matching on propensity scores. I'd imagine if I had instead used a different matching algorithm that matches on propensity scores, like genetic matching, I might find that the alternative method had more runs with higher proportions of balanced covariates. I'm not sure if it's that the Mahalanobis matching is less 'sophisticated,' but perhaps it suggests that our propensity scores are useful?
    \item \textbf{Use a visualization to examine the change in the distribution of the percent improvement in balance in propensity score matching vs. the distribution of the percent improvement in balance in your new method. Which did better? Analyze the results in 1-2 sentences.}
    Your Answer: See above. Adding that the Mahalanoobis matching method seems to also have more extreme values in terms of the percent improvement at both ends.
\end{enumerate}

\textbf{Optional:} Looking ahead to the discussion questions, you may choose to model the propensity score using an algorithm other than logistic regression and perform these simulations again, if you wish to explore the second discussion question further.

# Discussion Questions

\begin{enumerate}
    \item \textbf{Why might it be a good idea to do matching even if we have a randomized or as-if-random design?}
    Your Answer:As discussed above, even if we're using a randomization method, we can still have imbalances in our covariates, which we saw in our analyses. Matching can work in tandem with randomization to make sure that covariates are balanced across treatment and control groups, so that we can be more sure that the treatment effect we're seeing is attributable to the treatment itself, rather than a covariate. 
    \item \textbf{The standard way of estimating the propensity score is using a logistic regression to estimate probability of treatment. Given what we know about the curse of dimensionality, do you think there might be advantages to using other machine learning algorithms (decision trees, bagging/boosting forests, ensembles, etc.) to estimate propensity scores instead?}
    Your Answer: As we've talked about in previous projects, logistic regression relies upon the assumption that there's a linear relationship between our covariates and the treatment. We can see even from the many covariates we had in this project that this might not be true to real life. I'm thinking that maybe random forests would be an interesting way of exploring the relationship between covariates and how they relate to probability of treatment because random forests (and ML algorithms in general) can be less prescriptive about the relationship between variables/features than we were here. 
\end{enumerate}